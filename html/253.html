<!DOCTYPE html>



<html lang="en">
	<head>
		<meta charset="utf-8">	
		<meta name="viewport" content="width=device-width, initial_scale=1">
		<title>DH 2016 Abstracts</title>
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css">
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="/static/css/abstracts.css">
		<script src="/static/jquery/jquery-2.2.4.min.js"></script>
		<script src="/static/bootstrap/js/bootstrap.min.js"></script>
	</head>
	<body>
		<div class="container">
			<div class="row">
				<a href="http://dh2016.adho.org"><img src="/static/images/logo_4.png" class="img-responsive"></a>
			</div>
			<div class="row">
				<ol class="breadcrumb">
					
						<li><a href="http://www.dh2016.adho.org">DH Home</a></li>
					
						<li><a href="/abstracts/">Abstracts</a></li>
					
						<li><a href="/abstracts/253">253</a></li>
					
				</ol>
			</div>
					
			
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8 col-xs-12">
			<button class="btn btn-default" data-toggle="collapse" data-target="#head" style="margin-bottom: 5px;">Show info</button>
			<button class="btn btn-default" data-toggle="collapse" data-target="#cite" style="margin-bottom: 5px;">How to cite</button>
			<a href="/static/data/290.xml" class="btn btn-default" role="button" style="margin-bottom: 5px;" download="253.xml">XML Version</a>
			<div class="panel panel-success collapse" style="padding: 10px; border=none;" id="head">
				<div>
					<b>Title: </b>Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution
					<br>
					<b>Authors: </b>
					Stefan Evert, Fotis Jannidis, Thomas Proisl, Thorsten Vitt, Christof Schöch, Steffen Pielström, Isabella Reger
					<br>
					<b>Category: </b>Paper:Long Paper
					<br>
					<b>Keywords: </b>Delta, distance measures, normalization
				</div>
			</div>
			<div class="collapse" id="cite">
				<b>Evert, S., Jannidis, F., Proisl, T., Vitt, T., Schöch, C., Pielström, S., Reger, I.</b> (2016). Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution. In <i>Digital Humanities 2016: Conference Abstracts</i>. Jagiellonian University & Pedagogical University, Kraków, 
				
					pp. 188-191.
				
			</div>
		</div>
		<div class="col-lg-2">
		</div>
	</div>
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8">	
			<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)--><title>Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution </title><meta name="author" content="Stefan Evert , Fotis Jannidis , Thomas Proisl , Thorsten Vitt , Christof Schöch , Steffen Pielström , and Isabella Reger" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta name="DC.Title" content="Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution" /><meta name="DC.Type" content="Text" /><meta name="DC.Format" content="text/html" /><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css" /><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css" /></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle"><span class="titlem">Outliers or Key Profiles? Understanding Distance Measures for Authorship Attribution</span> <span class="titlem"></span></h1></div><!--TEI front--><!--TEI body--><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">The state of the art</span></h2><p><span style="color:#000000">Burrows’ Delta is one of the most successful algorithms in computational stylistics (Burrows 2002). A series of studies have proven its usefulness (e.g. Hoover 2004, Rybicki &amp; Eder 2011). There are two essential steps in Burrows’ Delta. The first is to standardize the relative frequencies of words in a document-term-matrix through a </span> <span style="color:#000000">z-score</span> <span style="color:#000000"> </span> <span style="color:#000000">transformation. In the second step, the distances between all texts are calculated. For each word, the difference between the </span> <span style="color:#000000">z-score</span> <span style="color:#000000"> </span> <span style="color:#000000">of the word in one and the other text are calculated. The absolute values of the differences are added for all words taken into account. The usual interpretation is that the smaller the sum, the more similar two texts are stylistically, and the more likely it is that they have been written by the same author. </span></p><p>Despite the fact that Burrows’ Delta is as simple as it is useful, there is still a lack of a good explanation why the algorithm works so well. Argamon (2002) has shown that the second step in Burrows’ Delta is equivalent to taking the Manhattan distance between two points in a multi-dimensional space. He suggests, among other things, using the Euclidean distance instead. An empirical test of his proposals has shown, however, that none of them lead to an improvement in performance (Jannidis et al. 2015).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007AF000007CFCCC81279FE2EA7FD.png" alt="Figure 1: Illustration of the distance between two texts made up of just two words" class="graphic" /><div class="caption">Figure 1: Illustration of the distance between two texts made up of just two words</div></div><p>Smith and Aldrige (2011) have suggested to use the cosine of the angle between the document vectors for the second step, as is customary in Information Retrieval (Baeza-Yates &amp; Ribeiro-Neto 1999:27). The Cosine variant of Delta (Delta <span style="font-style:italic">Cos</span>) outperforms Burrows’ Delta (Delta <span style="font-style:italic">Bur</span>) in many different settings and has the advantage of not showing the drop in performance typical ofother Delta variants when large numbers of MFW are used (Jannidis et al. 2015). The question now is why Delta <span style="font-style:italic">Cos</span> is so much better than Delta <span style="font-style:italic">Bur</span> and other variants, that is, in what way Delta <span style="font-style:italic">Cos</span> captures the authorship signal more clearly than other variants of Delta.</p><p>Of decisive importance for our further analyses was the insight that using the Cosine Distance is equivalent to a vector normalization in the sense that (in contrast to Manhattan and Euclidean Distance) the length of the vector does not play a role for the calculation of the distance (see figure 1). Previous experiments have shown that an explicit, additional vector normalization also substantially improves performance of the other Delta measures (Evert et al. 2015).</p></div><div class="DH-Heading1" id="index.xml-body.1_div.2"><h2 class="DH-Heading1"><span class="headingNumber">2. </span><span class="head">Hypotheses</span></h2><p>Having discovered that impact of the normalization effect, we have developed two empirically testable hypotheses:</p><ul><li class="item">(H1) Performance differences are caused by single extreme values, so-called outliers. These are particularly large positive or negative <span style="font-style:italic">z-scores</span> specific to single texts rather than all texts of a single author. As the Euclidean distance should be more sensitive to single extreme values than the Manhattan distance, this hypothesis would explain the comparatively bad performance of Argamon’s “Quadratic Delta” Delta <span style="font-style:italic">Q</span>. The positive effect of vector normalization originates from the reduction of outlier amplitudes (“outlier hypothesis”).</li><li class="item">(H2) The author specific “style profile” manifests itself more in the qualitative combination of word preferences, i.e. in the pattern of over- and under utilization of vocabulary, rather than in the actual amplitude of <span style="font-style:italic">z-scores</span>. A text distance measure is particularly successful in authorship attribution if emphasizing structural differences of author style profiles without being too much influenced by actual amplitudes (“key-profile hypothesis”). This hypothesis explains directly why vector normalization results in such impressive improvements: it standardizes the amplitudes of author profiles in different texts.</li></ul></div><div class="DH-Heading1" id="index.xml-body.1_div.3"><h2 class="DH-Heading1"><span class="headingNumber">3. </span><span class="head">New insights</span></h2><div class="DH-Heading2" id="index.xml-body.1_div.3_div.1"><h3 class="DH-Heading2"><span class="headingNumber">3.1. </span><span class="head">Corpora</span></h3><p>For the experiments in this paper, we use three similarly composed corpora in German, English and French. Each corpus contains 25 different authors with 3 novels each, thus 75 texts in total. The corpora have been described in Jannidis et al. (2015). Due to space issues, the following section will only present our observations on the German corpus. The results for the corpora in both other languages show only small deviations and also support our findings.</p></div><div class="DH-Heading2" id="index.xml-body.1_div.3_div.2"><h3 class="DH-Heading2"><span class="headingNumber">3.2. </span><span class="head">Experiments</span></h3><p>To further investigate the role of outliers and thus the plausibility of H1, we complement Delta <span style="font-style:italic">Bur</span> and Delta <span style="font-style:italic">Q</span> with additional variants based on the general Minkowski distance (for <span style="font-style:italic">p </span>≥ 1):</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/1000020100000107000000361C04104BBC97A252.png" alt="" class="graphic" /></div><p>We generally name these distance measures L <span style="font-style:italic">p</span>-Delta. The specific case <span style="font-style:italic">p</span> = 1 equals the Manhattan distance (L <span style="font-style:italic">1</span>-Delta = Delta <span style="font-style:italic">Bur</span>), <span style="font-style:italic">p</span> = 2 the Euclidean distance (L <span style="font-style:italic">2</span>-Delta = Delta <span style="font-style:italic">Q</span>). The higher the value for <span style="font-style:italic">p</span>, the larger the influence of single outliers on L <span style="font-style:italic">p</span>-Delta.</p><p>Fig. 2 compares four different L <span style="font-style:italic">p</span> distance measures (for p=1, √2, 2, 4) with Delta <span style="font-style:italic">Cos</span>. The method of comparison is the same as in Evert et al. (2015): 75 text are automatically clustered in 25 groups according to Delta distances; clustering quality is estimated with the adjusted rand index (ARI). An ARI of 100% signifies perfect author recognition whereas a value of 0% shows that the clustering is entirely random. The performance of L <span style="font-style:italic">p</span> Delta obviously decreases with increasing <span style="font-style:italic">p</span>. Additionally, the robustness of the measures also decreases with an increasing number of MWF used. As already reported in Jannidis et al. (2015) and Evert et al. (2015), Delta <span>Bur</span> (L <span>1</span>) consistently outperforms Argamon’s Delta <span style="font-style:italic">Q</span> (L <span style="font-style:italic">2</span>). Especially if many features, i.e. a large number of MFW is considered, high p values result in low performance. Delta <span style="font-style:italic">Cos</span> is more robust than other variants and achieves almost perfect attribution success (ARI &gt; 90%) over a wide range of the MFW.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007CF0000039D7A13BB39B37D0A44.png" alt="Figure 2: Clustering quality of different Delta measures as a function of the number of the MFW considered" class="graphic" /><div class="caption">Figure 2: Clustering quality of different Delta measures as a function of the number of the MFW considered</div></div><p>Normalizing the feature vectors to length 1 improves the quality of all Delta measures significantly (fig. 3). In this case, Argamon’s Delta <span style="font-style:italic">Q</span> is identical to Delta <span style="font-style:italic">Cos</span>: the red line is completely covered by the green one. The other Delta measures (Delta <span style="font-style:italic">Bur</span>, L <span style="font-style:italic">1.4</span>-Delta) now reach about the same quality as Delta <span style="font-style:italic">Cos</span>. Only L <span style="font-style:italic">4</span> Delta, which is especially prone to outliers, falls short considerably. These results seem to support H1.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007CF0000039D4B5D688B276087C2.png" alt="Figure 3: Cluster quality of various Delta measures with length-normalized vectors" class="graphic" /><div class="caption">Figure 3: Cluster quality of various Delta measures with length-normalized vectors</div></div><p>A different approach to limit the influence of outliers is to truncate extreme <span style="font-style:italic">z-scores</span>. To do so, we set all | <span style="font-style:italic">z</span>| &gt; 2 to +2 or –2, depending on the original <span style="font-style:italic">z-scores</span>’s sign. Fig. 4 shows the effects of various normalizations on the distribution of the feature values. Vector length normalization (lower left) produces only slight changes and practically does not reduce the number of outliers at all. Pruning large <span style="font-style:italic">z-score</span> values only affects words with above-average frequencies (upper right).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007CF0000051AA61625924867E17C.png" alt="Figure 4: Distributions of feature vectors for all 75 texts, using vectors of 5000 most frequent words. The table shows the distribution of the original &#xA;                                 (upper left), the distribution after length-normalizing the vectors (lower left), the distribution after clamping outliers with |&#xA;                                | &gt; 2 (upper right) and a ternary quantization to the values –1, 0 and +1 (lower right). The red curve in the lower left graph shows the &#xA;                                 before normalization; the direct comparison shows the normalization has only minimal effect and almost does not reduce outliers. The thresholds for the ternary quantization, &#xA;                                 &lt; –0.43 (–1), –0.43 ≤ &#xA;                                 ≤ 0.43 (0) and &#xA;                                 &gt; 0.43 (+1), have been selected such that in an ideal normal distribution, a third of all feature values would fall into each of the classes –1, 0, and +1." class="graphic" /><div class="caption">Figure 4: Distributions of feature vectors for all 75 texts, using vectors of 5000 most frequent words. The table shows the distribution of the original <span style="font-style:italic">z-scores</span> (upper left), the distribution after length-normalizing the vectors (lower left), the distribution after clamping outliers with | <span style="font-style:italic">z</span>| &gt; 2 (upper right) and a ternary quantization to the values –1, 0 and +1 (lower right). The red curve in the lower left graph shows the <span style="font-style:italic">z-scores</span> before normalization; the direct comparison shows the normalization has only minimal effect and almost does not reduce outliers. The thresholds for the ternary quantization, <span style="font-style:italic">z</span> &lt; –0.43 (–1), –0.43 ≤  <span style="font-style:italic">z</span> ≤ 0.43 (0) and <span style="font-style:italic">z</span> &gt; 0.43 (+1), have been selected such that in an ideal normal distribution, a third of all feature values would fall into each of the classes –1, 0, and +1.</div></div><p> </p><p>As Fig. 5 shows, this manipulation improves the performance of all L <span style="font-style:italic">p</span> Deltas considerably. However, its positive effect is noticeably smaller than that of vector normalization.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007CF0000039D8C2EE8D70E074D6B.png" alt="Figure 5: Cluster quality after clamping outliers, i.e. feature values with |&#xA;                                | &gt; 2 have been replaced with the fixed values –2 or +2, depending on &#xA;                                ’s sign" class="graphic" /><div class="caption">Figure 5: Cluster quality after clamping outliers, i.e. feature values with | <span style="font-style:italic">z</span>| &gt; 2 have been replaced with the fixed values –2 or +2, depending on <span style="font-style:italic">z-score</span>’s sign</div></div><p>With these differing effects of the normalizations on outlier distributions and Delta results, H1 cannot be upheld. H2 is supported by the good results of vector length normalization. However, on its own, it cannot explain why clamping outliers leads to a considerable improvement as well. To examine this hypothesis further, we created pure “key profile” vectors that only discriminate between word frequencies that are above average (+1), unremarkable (0), and below average (–1; cf. Fig. 4, lower right).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/290/10000201000007CF0000039DBF7F231C631CC80A.png" alt="Figure 6: Cluster quality with ternary quantization of the vectors in frequencies that are above average (+1, &#xA;                                 &gt; 0.43), unremarkable (0, –0.43 ≤ &#xA;                                 ≤ 0.43), and below average (&#xA;                                 &lt; –0.43)" class="graphic" /><div class="caption">Figure 6: Cluster quality with ternary quantization of the vectors in frequencies that are above average (+1, <span style="font-style:italic">z</span> &gt; 0.43), unremarkable (0, –0.43 ≤  <span style="font-style:italic">z</span> ≤ 0.43), and below average ( <span style="font-style:italic">z</span> &lt; –0.43)</div></div><p>Fig. 6 shows that these key profile vectors perform remarkably well, almost on par with vector normalization. Even the especially outlier-prone L <span style="font-style:italic">4</span> Delta reaches a quite robust clustering quality of more than 90%. We interpret this observation as giving considerable support to hypothesis H2.</p></div></div><div class="DH-Heading1" id="index.xml-body.1_div.4"><h2 class="DH-Heading1"><span class="headingNumber">4. </span><span class="head">Discussion and perspectives</span></h2><p>H1, the outlier hypothesis, has been disproven as the vector normalisation hardly reduces the number of extreme values and the quality of all L <span style="font-style:italic">p</span> measures is still considerably improved. On the other hand, H2, the key profile hypothesis, has been confirmed. The ternary quantification of the vectors shows clearly that it is not the extent of deviation resp. the size of the amplitude, but the profile of deviation across the MFW which is important. Remarkably, the measures behave differently if more than 2000 MFW are used. Almost all variant show a decline for a very large number of features, but they differ in when this decline starts. We suppose that the vocabulary in those parts is less specific for an author than for topics and content. Clarifying such questions will require further experiments.</p></div><!--TEI back--><div class="bibliogr" id="index.xml-back.1_div.1"><h2><span class="headingNumber"> </span></h2><div class="listhead">Bibliography</div><ol class="listBibl"><li id="index.xml-bibl-d30e631"><div class="biblfree"><span style="font-weight:bold">Argamon, S.</span> (2008). Interpreting Burrows’s Delta: Geometric and Probabilistic Foundations. <span style="font-style:italic">Literary and Linguistic Computing</span>, <span style="font-weight:bold">23</span>(2): 131–47 doi:10.1093/llc/fqn003. <a class="link_ptr" href="http://llc.oxfordjournals.org/content/23/2/131.abstract"><span>http://llc.oxfordjournals.org/content/23/2/131.abstract</span></a>.</div></li><li id="index.xml-bibl-d30e645"><div class="biblfree"><span style="font-weight:bold">Baeza-Yates, R. and Ribeiro Neto, B.</span> (1999). <span style="font-style:italic">Baeza-Yates, Ricardo; Ribeiro Neto, Berthier (1999): Modern Information Retrieval. Harlow.</span> Harlow.</div></li><li id="index.xml-bibl-d30e654"><div class="biblfree"><span style="font-weight:bold">Burrows, J.</span> (2002). ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. <span style="font-style:italic">Literary and Linguistic Computing</span>, <span style="font-weight:bold">17</span>(3): 267–87 doi:10.1093/llc/17.3.267. <a class="link_ptr" href="http://llc.oxfordjournals.org/content/17/3/267.abstract"><span>http://llc.oxfordjournals.org/content/17/3/267.abstract</span></a>.</div></li><li id="index.xml-bibl-d30e668"><div class="biblfree"><span style="font-weight:bold">Eder, M. and Rybicki, J.</span> (2011). Deeper Delta across genres and languages: do we really need the most frequent words?. <span style="font-style:italic">Literary and Linguistic Computing</span>, <span style="font-weight:bold">26</span>(3): 315–21 doi:10.1093/llc/fqr031. <a class="link_ptr" href="http://llc.oxfordjournals.org/content/early/2011/07/14/llc.fqr031.abstract"><span>http://llc.oxfordjournals.org/content/early/2011/07/14/llc.fqr031.abstract</span></a> .</div></li><li id="index.xml-bibl-d30e683"><div class="biblfree"><span style="font-weight:bold">Evert, S., Proisl, T., Pielström, S., Schöch, C. and Vitt, T.</span> (2015). Towards a better understanding of Burrows’s Delta in literary authorship attribution. <span style="font-style:italic">Proceedings of the Fourth Workshop on Computational Linguistics for Literature</span>. Denver CO.</div></li><li id="index.xml-bibl-d30e692"><div class="biblfree"><span style="font-weight:bold">Hoover, D. L.</span> (2004). Testing Burrows’s Delta. <span style="font-style:italic">Literary and Linguistic Computing</span>, <span style="font-weight:bold">19</span>(4): 453–75 doi:10.1093/llc/19.4.453. <a class="link_ptr" href="http://llc.oxfordjournals.org/content/19/4/453.abstract"><span>http://llc.oxfordjournals.org/content/19/4/453.abstract</span></a>.</div></li><li id="index.xml-bibl-d30e706"><div class="biblfree"><span style="font-weight:bold">Jannidis, F., Pielström, S., Schöch, C. and Vitt, T.</span> (2015). Improving Burrows’ Delta – An empirical evaluation of text distance measures. <span style="font-style:italic">Digital Humanities 2015 Conference Abstracts</span>. Sydney: ADHO <a class="link_ptr" href="http://dh2015.org/abstracts/xml/JANNIDIS_Fotis_Improving_Burrows__Delta___An_empi/JANNIDIS_Fotis_Improving_Burrows__Delta___An_empirical_.html"><span>http://dh2015.org/abstracts/xml/JANNIDIS_Fotis_Improving_Burrows__Delta___An_empi/JANNIDIS_Fotis_Improving_Burrows__Delta___An_empirical_.html</span></a>.</div></li><li id="index.xml-bibl-d30e717"><div class="biblfree"><span style="font-weight:bold">Smith, P. W. H. and Aldridge, W.</span> (2011). Improving Authorship Attribution: Optimizing Burrows’ Delta Method. <span style="font-style:italic">Journal of Quantitative Linguistics</span>, <span style="font-weight:bold">18</span>(1): 63–88 doi:10.1080/09296174.2011.533591. <a class="link_ptr" href="http://www.tandfonline.com/doi/abs/10.1080/09296174.2011.533591"><span>http://www.tandfonline.com/doi/abs/10.1080/09296174.2011.533591</span></a>.</div></li></ol></div></body></html>
		</div>
		<div class="col-lg-2">
		</div>
	</div>


		</div>
		<script>
		$(document).ready(function(){
			$('[data-toggle="tooltip"]').tooltip();
		});
		</script>			
	</body>
</html>