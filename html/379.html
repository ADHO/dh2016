<!DOCTYPE html>



<html lang="en">
	<head>
		<meta charset="utf-8">	
		<meta name="viewport" content="width=device-width, initial_scale=1">
		<title>DH 2016 Abstracts</title>
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css">
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="/static/css/abstracts.css">
		<script src="/static/jquery/jquery-2.2.4.min.js"></script>
		<script src="/static/bootstrap/js/bootstrap.min.js"></script>
	</head>
	<body>
		<div class="container">
			<div class="row">
				<a href="http://dh2016.adho.org"><img src="/static/images/logo_4.png" class="img-responsive"></a>
			</div>
			<div class="row">
				<ol class="breadcrumb">
					
						<li><a href="http://www.dh2016.adho.org">DH Home</a></li>
					
						<li><a href="/abstracts/">Abstracts</a></li>
					
						<li><a href="/abstracts/379">379</a></li>
					
				</ol>
			</div>
					
			
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8 col-xs-12">
			<button class="btn btn-default" data-toggle="collapse" data-target="#head" style="margin-bottom: 5px;">Show info</button>
			<button class="btn btn-default" data-toggle="collapse" data-target="#cite" style="margin-bottom: 5px;">How to cite</button>
			<a href="/static/data/416.xml" class="btn btn-default" role="button" style="margin-bottom: 5px;" download="379.xml">XML Version</a>
			<div class="panel panel-success collapse" style="padding: 10px; border=none;" id="head">
				<div>
					<b>Title: </b>Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities
					<br>
					<b>Authors: </b>
					Rui Hu, Jean-Marc Odobez, Daniel Gatica-Perez
					<br>
					<b>Category: </b>Paper:Long Paper
					<br>
					<b>Keywords: </b>Shape descriptor, image retrieval, Maya hieroglyph
				</div>
			</div>
			<div class="collapse" id="cite">
				<b>Hu, R., Odobez, J., Gatica-Perez, D.</b> (2016). Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities. In <i>Digital Humanities 2016: Conference Abstracts</i>. Jagiellonian University & Pedagogical University, Kraków, 
				
					pp. 225-229.
				
			</div>
		</div>
		<div class="col-lg-2">
		</div>
	</div>
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8">	
			<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)--><title>Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities </title><meta name="author" content="Rui Hu , Jean-Marc Odobez and Daniel Gatica-Perez" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta name="DC.Title" content="Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities" /><meta name="DC.Type" content="Text" /><meta name="DC.Format" content="text/html" /><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css" /><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css" /></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle"><span class="titlem">Assessing a Shape Descriptor for Analysis of Mesoamerican Hieroglyphics: A View Towards Practice in Digital Humanities</span> <span class="titlem"></span></h1></div><!--TEI front--><!--TEI body--><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">Introduction</span></h2><p>Technological advances in digitization, automatic image analysis and information management are enabling the possibility to analyze, organize and visualize large cultural datasets. As one of the key visual cues, shape feature has been used in various image analysis tasks such as handwritten character recognition (Fischer, 2012; Franken, 2013), sketch analysis (Eitz, 2012), etc. We assess a shape descriptor, within the application domain of Maya hieroglyphic analysis. Our aim is to introduce this descriptor to the wider Digital Humanities (DH) community, as a shape analysis tool for DH-related applications.</p><p>The Maya civilization is one of the major cultural developments in ancient Mesoamerica. The ancient Maya language infused art with uniquely pictorial forms of hieroglyphic writing, which represents an exceptionally rich legacy (Stone, 2011). Most Maya texts were written during the Classic period (AD 250-900) of the Maya civilization on various media types, including stone monuments. A special class of Maya texts was written on bark cloths as folding books from the Post-Classic period (AD 1000-1519). Only three such books (namely the Dresden, Madrid and Paris codices) are known to have survived the Spanish Conquest. A typical Maya codex page contains icons, main sign glyph blocks, captions, calendric signs, etc. Fig. 1 illustrates an example page segmented into main elements (Gatica-Perez, 2014). In this paper, we are interested in the main signs.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/100000000000089800000647989F2083.jpg" alt="Figure 1. Page 6b of the Dresden Codex, showing individual constituting elements framed by blue rectangles (Hu, 2015), Green arrows indicate reading order of the main sign blocks, generated by Carlos Pallan based on SLUB (&#xA;                             ) online open source image.&#xA;                        " class="graphic" /><div class="caption">Figure 1. Page 6b of the Dresden Codex, showing individual constituting elements framed by blue rectangles (Hu, 2015), Green arrows indicate reading order of the main sign blocks, generated by Carlos Pallan based on SLUB ( <a class="link_ptr" href="http://digital.slub-dresden.de/werkansicht/dlf/2967/1/"><span>http://digital.slub-dresden.de/werkansicht/dlf/2967/1/</span></a> ) online open source image.</div></div><p>Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing existing catalogs to identify individual glyphs. Automatic Maya glyph analysis has been addressed as a shape matching problem, and a robust shape descriptor called Histogram of Orientation Shape Context (HOOSC) was developed in (Roman-Rangel, 2011). Since then, HOOSC has been successfully applied for automatic analysis of other cultural heritage data, such as Oracle-Bones Inscriptions of ancient Chinese characters (Roman-Rangel, 2012), and ancient Egyptian hieroglyphs (Franken, 2013). It has also been applied for generic sketch and shape image retrieval (Roman-Rangel, 2012). Our recent work extracted statistic Maya language model and incorporated it for glyph retrieval (Hu, 2015).</p><p>The goal of this paper is two-fold:</p><ol><li class="item">Introduce the HOOSC descriptor to be used in DH-related shape analysis tasks (code available at: <a class="link_ptr" href="http://www.idiap.ch/paper/maaya/hoosc/"><span>http://www.idiap.ch/paper/maaya/hoosc/</span></a>);</li><li class="item">Discuss key issues for practitioners, namely the effect that certain parameters have on the performance of the descriptor. We describe the impact of such choices on different data types, specially for 'noisy' data as it is often the case with DH image sources.</li></ol></div><div class="DH-Heading1" id="index.xml-body.1_div.2"><h2 class="DH-Heading1"><span class="headingNumber">2. </span><span class="head">Automatic Maya Hieroglyph Recognition</span></h2><p>We conduct glyph recognition with a retrieval system proposed in (Hu, 2015). Unknown glyphs are considered as queries to match with a database of known glyphs (retrieval database). Shape and context information are considered. Fig.2 illustrates a schema of our approach. We study the effect of different HOOSC parameter choices on the retrieval results.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/100002010000032300000180AC2EF00A.png" alt="Figure 2. Retrieval system pipeline." class="graphic" /><div class="caption">Figure 2. Retrieval system pipeline.</div></div><div class="DH-Heading2" id="index.xml-body.1_div.2_div.1"><h3 class="DH-Heading2"><span class="headingNumber">2.1. </span><span class="head">Datasets</span></h3><p>We use three datasets, namely the 'Codex', 'Monument' and 'Thompson'. The first two are used as queries to search within the retrieval database ('Thompson').</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/10000000000005F1000003A997F1124F.jpg" alt="Figure 3. Digitization quality: (left) raw glyph blocks cropped from Dresden codex; (middle) clean raster images produced by removing the background noise; (right) reconstructed high-quality vectorial images." class="graphic" /><div class="caption">Figure 3. Digitization quality: (left) raw glyph blocks cropped from Dresden codex; (middle) clean raster images produced by removing the background noise; (right) reconstructed high-quality vectorial images.</div></div><p>The 'Codex' dataset contains glyph blocks from the three surviving Maya codices. See Fig.3 for examples. Glyph blocks are typically composed of combinations of individual signs. Fig.4 shows individual glyphs segmented from blocks in Fig.3. Note the different degradation levels across samples. We use two sub-datasets: 'codex-small', composed of 156 glyphs segmented from 66 blocks, for which we have both clean raster and high-quality reconstructed vectorial representations (see Fig.4) to study the impact of the different data qualities on the descriptor; and a 'codex-large' dataset, which is more extensive, comprising only the raster representation of 600 glyphs from 229 blocks. </p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/100000000000057A000002D254172A9F.jpg" alt="Figure 4. Example glyph strings generated from blocks shown in Figure 3." class="graphic" /><div class="caption">Figure 4. Example glyph strings generated from blocks shown in Figure 3.</div></div><p>The 'Monument' dataset is an adapted version of the Syllabic Maya dataset used in (Roman-Rangel, 2011), which contains 127 glyphs of 40 blocks extracted from stone monuments. It is a quite different data source to the codex data, in terms of historical period, media type, and data generation process. Samples are shown in Fig.5. </p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/10000000000007670000025128EFE4C1.jpg" alt="Figure 5. Example blocks and segmented glyph strings form the 'Monument' dataset." class="graphic" /><div class="caption">Figure 5. Example blocks and segmented glyph strings form the 'Monument' dataset.</div></div><p>To form the retrieval database ('Thompson'), we scanned and segmented all the glyphs from the Thompson catalog (Thompson, 1962). The database contains 1487 glyph examples of 892 different sign categories. Each category is usually represented by a single example image. Sometimes multiple examples are included; each illustrates a different visual instance or a rotation variant. Fig.6 shows glyph examples. </p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/100000000000074C0000016D4A6642BE.jpg" alt="Figure 6. Thompson numbers, visual examples, and the syllabic values of glyph pairs. Each pair contains two different signs with similar visual features (Hu, 2015). All examples are taken from (Thompson, 1962)." class="graphic" /><div class="caption">Figure 6. Thompson numbers, visual examples, and the syllabic values of glyph pairs. Each pair contains two different signs with similar visual features (Hu, 2015). All examples are taken from (Thompson, 1962).</div></div></div><div class="DH-Heading2" id="index.xml-body.1_div.2_div.2"><h3 class="DH-Heading2"><span class="headingNumber">2.2. </span><span class="head">Shape-based retrieval</span></h3><p>Feature extraction and similarity matching are the two main steps for our shape-based glyph retrieval framework. </p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/1000000000000881000004C3545452A8.jpg" alt="Figure 7. Extracting HOOSC descriptor: (a) input clean raster image; (b) binary image; (c) thinned edge of (b); (d) reconstructed vector representation of (a); (e) thinned edge of (d); (f) corresponding groundtruth image in the catalog; (g)-(k) spatial partition of a same pivot point with five different ring sizes (1, ½, ¼, 1/8, 1/16, all defined as a proportion to the mean of the pairwise distance between pivot points) on the local orientation field of the thinned edge image (c). Note that we zoomed in to show the spatial context of 1/16 in (k)." class="graphic" /><div class="caption">Figure 7. Extracting HOOSC descriptor: (a) input clean raster image; (b) binary image; (c) thinned edge of (b); (d) reconstructed vector representation of (a); (e) thinned edge of (d); (f) corresponding groundtruth image in the catalog; (g)-(k) spatial partition of a same pivot point with five different ring sizes (1, ½, ¼, 1/8, 1/16, all defined as a proportion to the mean of the pairwise distance between pivot points) on the local orientation field of the thinned edge image (c). Note that we zoomed in to show the spatial context of 1/16 in (k).</div></div><p>Glyphs are first pre-processed into thin lines. To do so, an input glyph (Fig.7(a)) is first converted into a binary shape (Fig.7 (b)). Thin lines (Fig.7(c)) are then extracted through mathematical morphology operations. Fig.7(c)-(d) show the high-quality reconstructed binary image, and the extracted thin lines. </p><p>HOOSC descriptors are then computed at a subset of uniformly sampled pivot points along the thin lines. HOOSC combines the strength of Histogram of Orientation Gradient (Dalal, 2005) with circular split binning from the shape context descriptor (Belongie, 2002). Given a pivot point, the HOOSC is computed on a local circular space centred at the pivot's location, partitioned into rings and evenly distributed angles. Fig.7 (g)-(k) show different sizes of the circular space (referred to as spatial context) partitioned into 2 rings and 8 orientations. A Histogram-of-orientation-gradient is calculated within each region. The HOOSC descriptor for a given pivot is the concatenation of histograms of all partitioned regions.</p><p>We then follow the Bag-of-Words (BoW) approach, where descriptors are quantized as visual words based on the vocabulary obtained through K-means clustering on the set of descriptors extracted from the retrieval database. A histogram representing the count of each visual word is then computed as a global descriptor for each glyph. In all experiments, we use vocabulary size k=5000.</p><p>Each query is matched with glyphs in the retrieval database, by computing shape feature similarity using the L1 norm distance.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/100000000000074D0000020A8E000B05.png" alt="Figure 8. Six pairs of glyph signs (Hu, 2015). Each pair contains a query glyph from the 'Codex' dataset (right), and their corresponding groundtruth in the catalog (left)." class="graphic" /><div class="caption">Figure 8. Six pairs of glyph signs (Hu, 2015). Each pair contains a query glyph from the 'Codex' dataset (right), and their corresponding groundtruth in the catalog (left).</div></div></div><div class="DH-Heading2" id="index.xml-body.1_div.2_div.3"><h3 class="DH-Heading2"><span class="headingNumber">2.3. </span><span class="head">Incorporating context information</span></h3><p>Shape alone is often ambiguous to represent and distinguish between images. In the case of our data, different signs often share similar visual features (see Fig.6); glyphs of the same sign category vary with time, location, and the different artists who produced them (see Fig.8); additionally, surviving historical scripts often lose visual quality over time. Context information can be used to complement the visual features.</p><p>Glyph co-occurrence within single blocks encodes valuable context information. To utilize this information, we arrange glyphs within a single block into a linear string according to the reading order (see Fig.4 and Fig.5), and consider the co-occurrence of neighbouring glyphs using an analogy to a statistical language model. For each unknown glyph in the string, we compute its probability to be labelled as a given category by considering not only the shape similarity, but also the compatibility to the rest of the string.</p><p>We apply the two language models extracted in (Hu, 2015), namely the ones derived from the Maya Codices Database (Vail, 2013) and the Thompson catalog (Thompson, 1962), which we refer to as the 'Vail' and the 'Thompson' models. We use Vail model with smoothing factor α=0 for the 'Codex' data, and the Thompson model with α=0.2 for the 'Monument' data.</p></div></div><div class="DH-Heading1" id="index.xml-body.1_div.3"><h2 class="DH-Heading1"><span class="headingNumber">3. </span><span class="head">Experiments and Results</span></h2><p>Our aim is to demonstrate the effect of various HOOSC parameters on retrieval results.</p><div class="DH-Heading2" id="index.xml-body.1_div.3_div.1"><h3 class="DH-Heading2"><span class="headingNumber">3.1. </span><span class="head">Experimental setting</span></h3><p>We illustrate the effect of 3 key parameters:</p><ul><li class="item">Size of the spatial context region within which HOOSC is computed</li></ul><p>A larger region encodes more context information and therefore captures more global structure of the shape. However, in the case of image degradation, a larger region could contain more noise. We evaluate five different spatial contexts as shown in Fig.7(g)-(k). The circular space is distributed over 8 angular intervals. </p><ul><li class="item">Number of rings to partition the local circular region</li></ul><p>This parameter represents different partition details. We evaluate either 1 or 2 rings, the inner ring covers half the distance to the outer ring. Each region is further characterized by a 8-bin histogram of the local orientations.</p><ul><li class="item">Position information</li></ul><p>Relative position (i, j) of a pivot in the 2-D image plane can be concatenated to the corresponding HOOSC feature.</p></div><div class="DH-Heading2" id="index.xml-body.1_div.3_div.2"><h3 class="DH-Heading2"><span class="headingNumber">3.2. </span><span class="head">Results and discussion</span></h3><p>Fig.9 shows the average groundtruth ranking in the retrieval results with different parameter settings, on three query sets, <span style="font-style:italic">e.g.</span> 'Codex-large', 'Codex-small' and 'Monument'. Each query image usually has only one correct match (groundtruth) in the retrieval database. The smaller the average ranking value, the better the result. From Fig.9 we can see the following:</p><ul><li class="item">In most cases, the best results are achieved by using the largest spatial context, with finer partitioning details (2 rings in our case);</li><li class="item">When the location information is not considered, results show a general trend of improving with increasing ring sizes. However, the results are more stable when the position information is encoded, <span style="font-style:italic">e.g.</span> a smaller ring size can also achieve promising results when the location information is incorporated. This is particularly useful when dealing with noisy data, where a smaller ring size is preferred to avoid extra noise been introduced by a larger spatial context;</li><li class="item">The results do not benefit from a finer partition, when a small spatial context is considered. However, results improve with finer partitions, when the spatial context becomes larger. </li><li class="item">Position information is more helpful when a small spatial context is considered.</li></ul><p>Fig.10 shows example query glyphs and their top returned results.</p></div></div><div class="DH-Heading1" id="index.xml-body.1_div.4"><h2 class="DH-Heading1"><span class="headingNumber">4. </span><span class="head">Conclusion</span></h2><p>We have introduced the HOOSC descriptor to be used in DH-related shape analysis tasks. We discuss the effect of parameters on the performance of the descriptor. Experimental results on ancient Maya hieroglyph data from two different sources (codex and monument) suggest that a larger spatial context with finer partitioning detail usually leads to better results, while a smaller spatial context with location information is a good choice for noisy/damaged data. The code for HOOSC is available so DH researchers can test the descriptor for their own tasks.</p></div><div class="DH-Heading" id="index.xml-body.1_div.5"><h2 class="DH-Heading"><span class="headingNumber">5. </span><span class="head">Acknowledgement</span></h2><p>We thank Edgar Roman-Rangel for co-inventing the HOOSC algorithm and providing the original code. We also thank Carlos Pallán Gayol, Guido Krempel, and Jakub Spotak for producing the data used in this paper. Finally, we thank the Swiss National Science Foundation (SNSF) and the German Research Foundation (DFG) for their support, through the MAAYA project.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/10000000000007EA0000093853776226.jpg" alt="Figure 9. Retrieval results on each dataset, with various feature representation choices. (left) shape-base results; (right) incorporating glyph co-occurrence information." class="graphic" /><div class="caption">Figure 9. Retrieval results on each dataset, with various feature representation choices. (left) shape-base results; (right) incorporating glyph co-occurrence information.</div></div><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/416/1000000000000790000009C9BCD3C226.jpg" alt="" class="graphic" /></div><p>Figure 10. Example queries (first column) and their top returned retrieval results, ranked from left to right in each row. Groundtruth images are highlighted in green bounding boxes.</p></div><!--TEI back--><div class="bibliogr" id="index.xml-back.1_div.1"><h2><span class="headingNumber"> </span></h2><div class="listhead">Bibliography</div><ol class="listBibl"><li id="index.xml-bibl-d30e499"><div class="biblfree"><span style="font-weight:bold">Belongie, S., Malik, J., and Puzicha, J.</span> (2002). Shape Matching and Object Recognition using Shape Contexts. <span style="font-style:italic">PAMI.</span> pp. 509-22.</div></li><li id="index.xml-bibl-d30e508"><div class="biblfree"><span style="font-weight:bold">Dalal, N., and Triggs, B.</span> (2005). Histogram of Oriented Gradients for Human Detection. <span style="font-style:italic">In CVPR</span>. pp. 886-93.</div></li><li id="index.xml-bibl-d30e517"><div class="biblfree"><span style="font-weight:bold">Eitz, M., et al.</span> (2012). Sketch-based shape retrieval. <span style="font-style:italic">ACM Transactions on Graphics</span>. <span style="font-weight:bold">31</span>: 1-10.</div></li><li id="index.xml-bibl-d30e529"><div class="biblfree"><span style="font-weight:bold">Fischer, A., et al.</span> (2012). The HisDoc Project. Automatic Analysis, Recognition, and Retrieval of Handwritten Historical Documents for Digital Libraries. <span style="font-style:italic">InterNational and InterDisciplinary Aspects of Scholarly Editing.</span></div></li><li id="index.xml-bibl-d30e539"><div class="biblfree"><span style="font-weight:bold">Franken, M., and Gemert, J. C.</span> (2013). Automatic Egyptian Hieroglyph Recognition by Retrieving Images as Texts. <span style="font-style:italic">ACM MM</span>, pp. 765-68.</div></li><li id="index.xml-bibl-d30e548"><div class="biblfree"><span style="font-weight:bold">Gatica-Perez, D., et al.</span> (2014). The MAAYA Project: Multimedia Analysis and Access for Documentation and Decipherment of Maya Epigraphy. <span style="font-style:italic">Digital Humanities Conference.</span></div></li><li id="index.xml-bibl-d30e557"><div class="biblfree"><span style="font-weight:bold">Hu, R., et al.</span> (2015). Multimedia Analysis and Access of Ancient Maya Epigraphy: Tools to support scholars on Maya hieroglyphics. <span style="font-style:italic">IEEE Signal Processing Magazine</span>, pp. 75-84.</div></li><li id="index.xml-bibl-d30e566"><div class="biblfree"><span style="font-weight:bold">Roman-Rangel, E.</span> (2012). <span style="font-style:italic">Statistical Shape Descriptors for Ancient Maya Hieroglyphs Analysis.</span> PhD thesis, École Polytechnique Fédérale de Lausanne.</div></li><li id="index.xml-bibl-d30e575"><div class="biblfree"><span style="font-weight:bold">Roman-Rangel, E., et al.</span> (2011). Analyzing Ancient Maya Glyph Collections with Contextual Shape Descriptors. <span style="font-style:italic">IJCV</span>, pp. 101-17.</div></li><li id="index.xml-bibl-d30e584"><div class="biblfree"><span style="font-weight:bold">Stone, A.J. and Zender, M.</span> (2011). <span style="font-style:italic">Reading Maya Art: A Hieroglyphic Guide to Ancient Maya Painting and Sculpture</span>. Thames and Hudson Limited Publisher.</div></li><li id="index.xml-bibl-d30e594"><div class="biblfree"><span style="font-weight:bold">Thompson, J. E. S.</span> (1962). <span style="font-style:italic">A Catalog of Maya Hieroglyphs</span>. University of Oklahoma Press.</div></li><li id="index.xml-bibl-d30e603"><div class="biblfree"><span style="font-weight:bold">Vail, G., and Hernández, C.</span> (2013). <span style="font-style:italic">The Maya Codices Database</span>, Version 4.1. A website and database available at http://www.mayacodices.org/.</div></li></ol></div></body></html>
		</div>
		<div class="col-lg-2">
		</div>
	</div>


		</div>
		<script>
		$(document).ready(function(){
			$('[data-toggle="tooltip"]').tooltip();
		});
		</script>			
	</body>
</html>