<!DOCTYPE html>



<html lang="en">
	<head>
		<meta charset="utf-8">	
		<meta name="viewport" content="width=device-width, initial_scale=1">
		<title>DH 2016 Abstracts</title>
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css">
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="/static/css/abstracts.css">
		<script src="/static/jquery/jquery-2.2.4.min.js"></script>
		<script src="/static/bootstrap/js/bootstrap.min.js"></script>
	</head>
	<body>
		<div class="container">
			<div class="row">
				<a href="http://dh2016.adho.org"><img src="/static/images/logo_4.png" class="img-responsive"></a>
			</div>
			<div class="row">
				<ol class="breadcrumb">
					
						<li><a href="http://www.dh2016.adho.org">DH Home</a></li>
					
						<li><a href="/abstracts/">Abstracts</a></li>
					
						<li><a href="/abstracts/250">250</a></li>
					
				</ol>
			</div>
					
			
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8 col-xs-12">
			<button class="btn btn-default" data-toggle="collapse" data-target="#head" style="margin-bottom: 5px;">Show info</button>
			<button class="btn btn-default" data-toggle="collapse" data-target="#cite" style="margin-bottom: 5px;">How to cite</button>
			<a href="/static/data/467.xml" class="btn btn-default" role="button" style="margin-bottom: 5px;" download="250.xml">XML Version</a>
			<div class="panel panel-success collapse" style="padding: 10px; border=none;" id="head">
				<div>
					<b>Title: </b>Wikidition: Towards A Multi-layer Network Model of Intertextuality
					<br>
					<b>Authors: </b>
					Alexander Mehler, Benno Wagner, Rüdiger Gleim
					<br>
					<b>Category: </b>Paper:Long Paper
					<br>
					<b>Keywords: </b>text mining, Wikidition, multi-layer network, linkification, lexiconisation
				</div>
			</div>
			<div class="collapse" id="cite">
				<b>Mehler, A., Wagner, B., Gleim, R.</b> (2016). Wikidition: Towards A Multi-layer Network Model of Intertextuality. In <i>Digital Humanities 2016: Conference Abstracts</i>. Jagiellonian University & Pedagogical University, Kraków, 
				
					pp. 276-279.
				
			</div>
		</div>
		<div class="col-lg-2">
		</div>
	</div>
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8">	
			<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)--><title>Wikidition: Towards A Multi-layer Network Model of Intertextuality </title><meta name="author" content="Alexander Mehler , Benno Wagner and Rüdiger Gleim" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta name="DC.Title" content="Wikidition: Towards A Multi-layer Network Model of Intertextuality" /><meta name="DC.Type" content="Text" /><meta name="DC.Format" content="text/html" /><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css" /><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css" /></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle"><span class="titlem">Wikidition: Towards A Multi-layer Network Model of Intertextuality</span> <span class="titlem"></span></h1></div><!--TEI front--><!--TEI body--><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">Introduction</span></h2><p>Current computational models of intertextuality run the risk of ignoring several desiderata: on the one hand, they mostly rely on single methods of quantifying text similarities. This includes syntagmatic models that look for shared vocabularies (unigram models) or (higher order) ( <span style="font-style:italic">k</span>-skip-) <span style="font-style:italic">n</span>-grams (Guthrie et al., 2006). Such approaches disregard the two-level process of sign constitution according to which language-related, paradigmatic relations have to be distinguished from their text-related, syntagmatic counterparts (Hjelmslev, 1969; Miller et al., 1991; Raible, 1981) where the former require language models of the sort of neural networks (Mikolov et al., 2013), topic models (Blei et al., 2007) or related approaches in the area of latent semantic analysis (cf., e.g., (Paaß et al., 2004)). On the other hand, computational models should enable scholars to revise their computations. The reason is the remarkably high error rate produced by statistical models even in cases that are supposed to be as “simple” as automatic <span style="font-style:italic">pre</span>-processing. Thus, scholars need efficient means to make numerous corrections and additions to automatic computations. Otherwise, the computations will be hardly acceptable as scientific data in the humanities (Thaller, 2014).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/467/10000201000005320000010C62AC686B05DB4FC0.png" alt="Table 1: Nine scenarios of generating Wikiditions out of corpora of (referring) literary (hyper-)texts and their (referred) hypotexts. Examples: (1) Kafka's &#34;Bericht für eine Akademie&#34; (in the role of a hypertext) versus Hauff's &#34;Der Affe als Mensch&#34; (in the role of a hypotext); (2) Kafka's &#34;Bericht für eine Akademie&#34; versus all &#34;Affentexte&#34; (Borgards, 2012) since the end of the 18th Century (including works of, e.g., Hauff, E. T. A. Hoffmann, Flaubert etc.); (3) Kafka's &#34;Beim Bau der Chinesischen Mauer&#34; versus the &#34;Prager Tagblatt&#34; from August 1914 to March 1917; (4) Kafka's &#34;Oeuvre&#34; versus Nietzsche's &#34;Geburt der Tragödie aus dem Geiste der Musik&#34;; (5) a selection of Kafka's &#34;Oeuvre&#34; versus a selection of Nietzsche's &#34;Oeuvre&#34;; (6) Kafka's &#34;Oeuvre&#34; versus a newspaper corpus (e.g., sampled from the &#34;Prager Tagblatt&#34;); (7) the complete works of several authors versus a single hypotext (e.g., Goethe's &#34;Faust&#34;); (8) the complete works of several authors versus a corpus of &#34;Faust&#34; texts; (9) the complete works of several German authors versus the complete works of several French authors." class="graphic" /><div class="caption">Table 1: Nine scenarios of generating Wikiditions out of corpora of (referring) literary (hyper-)texts and their (referred) hypotexts. Examples: (1) Kafka's "Bericht für eine Akademie" (in the role of a hypertext) versus Hauff's "Der Affe als Mensch" (in the role of a hypotext); (2) Kafka's "Bericht für eine Akademie" versus all "Affentexte" (Borgards, 2012) since the end of the 18th Century (including works of, e.g., Hauff, E. T. A. Hoffmann, Flaubert etc.); (3) Kafka's "Beim Bau der Chinesischen Mauer" versus the "Prager Tagblatt" from August 1914 to March 1917; (4) Kafka's "Oeuvre" versus Nietzsche's "Geburt der Tragödie aus dem Geiste der Musik"; (5) a selection of Kafka's "Oeuvre" versus a selection of Nietzsche's "Oeuvre"; (6) Kafka's "Oeuvre" versus a newspaper corpus (e.g., sampled from the "Prager Tagblatt"); (7) the complete works of several authors versus a single hypotext (e.g., Goethe's "Faust"); (8) the complete works of several authors versus a corpus of "Faust" texts; (9) the complete works of several German authors versus the complete works of several French authors.</div></div><p>This paper presents <span style="font-style:italic">Wikidition</span> as a <span style="font-style:italic">Literary Memory Information System</span> (LiMeS) to address these desiderata. It allows for the automatic generation of online editions of text corpora. This includes literary texts in the role of (referring) <span style="font-style:italic">hypertexts</span> (Genette, 1993) in relation to candidate (referred) hypotexts by exploring their <span style="font-style:italic">intra- and intertextual relations</span> – see Table 1 for nine related research scenarios. In order to explore, annotate and display such relations, Wikidition computes multi-layer networks that account for the multi-resolution of linguistic relations – on the side of the hypo- and the hypertexts. The reason is that hypertextual relations (in the sense of Genette) (that occur in the form of transformations, imitations or any mixture thereof) may be manifested on the lexical, sentential or the textual level (including whole paragraphs or even larger subtexts). As a consequence, Wikidition spans lexical, sentential and textual networks that allow for browsing along the constituency relations of words in relation to sentences, sentences in relation to texts etc. In this multi-layer network model, intrarelational links (of words, sentences or texts) are represented together with interrelational links that combine units of different layers. Figure 1 shows the range of sign relations that are mapped. To this end, Wikidition combines a multitude of text similarity measures (beyond <span style="font-style:italic">n</span>-grams) for automatically linking lexical, sentential and textual units regarding their (1) syntagmatic (e.g., syntactic) and (2) paradigmatic use. We call this two-level task <span style="font-style:italic">linkification</span>.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/467/10000201000005A800000271CE80EDC9FCC4FD2D.png" alt="Figure 1: Sign relations that are automatically explored and annotated by Wikidition (Mehler et al., 2016): on the level of words (Module (5) – paradigmatic –, (6) and (7) – both syntagmatic), on the level of sentences (Module (3) – paradigmatic – and (4) – syntagmatic) and on the level of texts (Module (1) and (2) – both paradigmatic). Wikidition additionally includes a component for wikification (i.e., for linking occurrences of concepts to articles in Wikipedia (Mihalcea et al., 2007)) and especially for automatically inducing lexica out of input corpora (i.e., for linkification). Arcs denote links explored by Wikidition; reflexive arcs denote intrarelational (i.e., purely lexical, sentential or textual) links. In this way, intra- and interrelational links are maintained by the same information system." class="graphic" /><div class="caption">Figure 1: Sign relations that are automatically explored and annotated by Wikidition (Mehler et al., 2016): on the level of words (Module (5) – paradigmatic –, (6) and (7) – both syntagmatic), on the level of sentences (Module (3) – paradigmatic – and (4) – syntagmatic) and on the level of texts (Module (1) and (2) – both paradigmatic). Wikidition additionally includes a component for wikification (i.e., for linking occurrences of concepts to articles in Wikipedia (Mihalcea et al., 2007)) and especially for automatically inducing lexica out of input corpora (i.e., for linkification). Arcs denote links explored by Wikidition; reflexive arcs denote intrarelational (i.e., purely lexical, sentential or textual) links. In this way, intra- and interrelational links are maintained by the same information system.</div></div><p>Beyond linkification, Wikidition contains a module for automatic <span style="font-style:italic">lexiconisation</span> (see Figure 2). It extracts lexica from input corpora to map author specific vocabularies as subsets of the corresponding reference language. Input corpora (currently in English, German or Latin) are given as plain text that first are automatically preprocessed; the resulting wikiditions are mapped onto separate URLs to be accessible as self-contained wikis. By means of lexiconisation, research questions of the following sort can be addressed: <span style="font-style:italic">What kind of German does Franz Kafka write? (E.g., Prager Deutsch.) What terminologies does Franz Kafka use in “In der Strafkolonie”? (E.g., engineering terminology.) How does his German depart from the underlying reference language?</span> Since texts are not necessarily monolingual (because of using citations, translations, loan words, verbal expressions etc.), the same procedure can be applied by simultaneously looking at all foreign languages being manifested in the texts under consideration (right side of Figure 2).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/467/10000201000003CE0000028AAB9CD0E53B655ADE.png" alt="Figure 2: Left side: schematic depiction (red) of the vocabulary of an author (e.g., Franz Kafka) as manifested within Wikidition's input text(s) (e.g., &#34;In der Strafkolonie&#34;) as mainly overlapping with the vocabulary of the corresponding reference language (e.g., German)." class="graphic" /><div class="caption">Figure 2: Left side: schematic depiction (red) of the vocabulary of an author (e.g., Franz Kafka) as manifested within Wikidition's input text(s) (e.g., "In der Strafkolonie") as mainly overlapping with the vocabulary of the corresponding reference language (e.g., German).</div></div><p>To this end, Wikidition distinguishes three levels of lexical resolution: superlemmas (e.g. German <span style="font-style:italic">Tätigkeit</span>), lemmas (e.g., <span style="font-style:italic">Thätigkeit</span>) and syntactic words (e.g., <span style="font-style:italic">Thätigkeiten</span> ( <span style="font-style:italic">nominative</span>, <span style="font-style:italic">plural</span>)) as featured sign-like manifestations of lemmas (lower part of the figure). Note that this model diverges from the majority of computational models to textual data which start from tokens as manifestations of wordforms (referred to as types) and which, therefore, disregard the meaning-side of lexical units. Based on linkification and lexiconisation, Wikidition does not only allow for traversing input corpora on different (lexical, sentential and textual) levels. Rather, the readers of a Wikidition can also study the vocabulary of single authors on several levels of resolution: starting from the level of superlemmas via the level of lemmas down to the level of syntactic words and wordforms (see Figure2).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/467/10000201000005720000021F41FDAD9F264C67A4.png" alt="Table 2: Notions of human, computer-supported and machine-based reading. Wikidition addressesmachine close reading by integrating semantic web (SW) resources and the human mind (HM) (asthe ultima ratio of interpreting its computations). T, ..., T span the input corpus of m (hyper-)texts; X denotes the contextualizing corpus of hypotexts of size n that is explicitly consulted by the readingprocess. Machine close reading is similar to human reading in that it focuses on small, rather than bigdata." class="graphic" /><div class="caption">Table 2: Notions of human, computer-supported and machine-based reading. Wikidition addressesmachine close reading by integrating semantic web (SW) resources and the human mind (HM) (asthe ultima ratio of interpreting its computations). T<span>1</span>, ..., T<span>m</span> span the input corpus of m (hyper-)texts; X<span>n</span> denotes the contextualizing corpus of hypotexts of size n that is explicitly consulted by the readingprocess. Machine close reading is similar to human reading in that it focuses on small, rather than bigdata.</div></div><p>While the linkification component of Wikidition relates to principles of WikiSource and Wikipedia, the Wiktionary project is addressed by its lexiconisation module. Wikidition uses numerous computational methods for providing interoperability and extensibility of the resulting editions according to the wiki principle. In this way, the dissemination of computer-based methods is supported even across the borders of digital humanities in that scholars are enabled to make their own exploratory analyses. However, Wikidition does not address a big data scenario in support of distant reading (Moretti, 2013), nor does it aim at emulating human reading in the sense of machine reading (Etzioni, 2007). Rather, Wikidition addresses what we call <span style="font-style:italic">machine close reading</span> in that it aims at massively supporting the process of (scientific or literary) reading by means of computational methods (see Table 2).</p></div><div class="div1" id="index.xml-body.1_div.2"><h2><span class="headingNumber">2. </span><span class="head">Evaluation</span></h2><p>We exemplify Wikidition by example of three pairs of text. Regarding the layers of lemmas and sentences, Table 3 shows that Wikidition generates extremely sparse networks (whose cohesion is below 1%) of high cluster values and short average geodesic distances in conjunction with largest connected components that comprise almost all lexical and sentential nodes. In this example, we compute paradigmatic associations among words by means of word2vec (Mikolov et al., 2013) while sentence similarities are computed by means of the centroids of the embeddings of their lexical constituents. Networks are filtered by focusing on the first three most similar neighbors of each node – obviously, this does not interfere with the small-world topology of the networks. Each pair of texts is additionally described regarding the subnetwork of syntactic words and sentences. This is done to account for the impact of inflection on networking. As a result, the networks are thinned out (cohesion is now at most 0.5%), but neither the sizes of the largest connected components nor the cluster and distances values are affected considerably. Obviously, differentiation leads to sparseness, but in a sense that the general topology is retained. By focusing on a single level of resolution (e.g., paradigmatic relations among words), sub-networks are generated that fit into what is known about universal laws of complex linguistic networks (Mehler, 2008). See (Mehler et al., 2016) for additional evaluations of Wikidition.</p><div class="table" id="Eval-Table"><table class="frame" style="border-collapse:collapse;border-spacing:0;"><caption>Table 3: Wikiditions of three text pairs (Kafka: Beim Bau der Chinesischen Mauer // Nietzsche: Die Zeit der Zyklopen-Bauten; Kafka: Ein Bericht für eine Akademie // Rathenau: Höre, Israel; Kafka: In der Strafkolonie // Rauchberg: Statistische Technik) compared by their cluster value c, the average geodesic distance of their nodes l, the fraction of nodes in their largest connected components lcc and by their cohesion coh (the number of links in relation to all possible links). First line of each text pair: nodes comprise lemmas and sentences; second line of each pair: nodes comprise syntactic words and sentences. Networking is conditioned by the operative preprocessor (Eger et al., 2016).</caption><tr><td>Edition</td><td>#nodes</td><td>#links</td><td class="italic">c</td><td class="italic">lcc</td><td class="italic">l</td><td class="italic">coh</td></tr><tr><td>Kafka // Nietzsche</td><td class="end">1624</td><td class="end">10391</td><td class="end">0,27</td><td class="end">1</td><td class="end">3,31</td><td class="end">0,008</td></tr><tr><td></td><td class="end">2401</td><td class="end">13644</td><td class="end">0,34</td><td class="end">1</td><td class="end">3,56</td><td class="end">0,005</td></tr><tr><td>Kafka // Rathenau</td><td class="end">1749</td><td class="end">10782</td><td class="end">0,28</td><td class="end">1</td><td class="end">3,36</td><td class="end">0,007</td></tr><tr><td></td><td class="end">2473</td><td class="end">13609</td><td class="end">0,36</td><td class="end">1</td><td class="end">3,60</td><td class="end">0,004</td></tr><tr><td>Kafka // Rauchberg</td><td class="end">4034</td><td class="end">30951</td><td class="end">0,25</td><td class="end">0,999</td><td class="end">3,35</td><td class="end">0,004</td></tr><tr><td></td><td class="end">6830</td><td class="end">44369</td><td class="end">0,31</td><td class="end">0,999</td><td class="end">3,62</td><td class="end">0,002</td></tr></table></div></div><div class="div1" id="index.xml-body.1_div.3"><h2><span class="headingNumber">3. </span><span class="head">Conclusion</span></h2><p>We presented Wikidition as a framework for exploring intra- and intertextual relations. Wikidition combines machine learning with principles of several wiki-based projects (Wikipedia, WikiSource and Wiktionary) to generate multi-layer networks from input corpora by integrating syntagmatic and paradigmatic relations on the lexical, sentential and the textual level. Our approach addresses intra- and inter-level networking in a single framework while adhering to laws of networking as being explored by complex network theory. In this way, input corpora get traversable in line with both empirical findings about characteristics of linguistic networks and the multi-resolution of sign relations whose space complexity is preferably reduced. Currently, Wikidition exists as a prototype that is further-developed by means of several edition projects in order to be finally made available as open source software. Wikidition is open for the cooperative development of digital editions.</p></div><!--TEI back--><div class="bibliogr" id="index.xml-back.1_div.1"><h2><span class="headingNumber"> </span></h2><div class="listhead">Bibliography</div><ol class="listBibl"><li id="index.xml-bibl-d30e530"><div class="biblfree"><span style="font-weight:bold">Eger, S. </span> <span style="font-weight:bold">and </span> <span style="font-weight:bold">Gleim, R. and Mehler, A.</span> (2016). Lemmatization and Morphological Tagging in German and Latin: A comparison and a survey of the state-of-the-art. In <span style="font-style:italic">Proceedings of the 10th International Conference on Language Resources and Evaluation</span>.</div></li><li id="index.xml-bibl-d30e545"><div class="biblfree"><span style="font-weight:bold">Etzioni, </span> <span style="font-weight:bold">O.</span> (2007). Machine reading of web text. In <span style="font-style:italic">Proceedings of the 4th International Conference on Knowledge Capture, K-CAP ’07</span>, pp. 1–4.</div></li><li id="index.xml-bibl-d30e557"><div class="biblfree"><span style="font-weight:bold">Genette, </span> <span style="font-weight:bold">G.</span> (1993). Palimpseste: Die Literatur auf zweiter Stufe. Suhrkamp, Frankfurt am Main.</div></li><li id="index.xml-bibl-d30e566"><div class="biblfree"><span style="font-weight:bold">Guthrie, </span> <span style="font-weight:bold">D., </span> <span style="font-weight:bold">Allison, </span> <span style="font-weight:bold">B., </span> <span style="font-weight:bold">Liu, </span> <span style="font-weight:bold">W., </span> <span style="font-weight:bold">Guthrie, </span> <span style="font-weight:bold">L.</span> <span style="font-weight:bold">and Wilks, </span> <span style="font-weight:bold">Y.</span> (2006). A closer look at skip-gram modelling.</div></li><li id="index.xml-bibl-d30e601"><div class="biblfree"><span style="font-weight:bold">Hjelmslev, </span> <span style="font-weight:bold">L.</span> (1969). Prolegomena to a Theory of Language. University of Wisconsin Press, Madison.</div></li><li id="index.xml-bibl-d30e610"><div class="biblfree"><span style="font-weight:bold">Kafka, </span> <span style="font-weight:bold">F.</span> (1916). Die Verwandlung. Kurt Wolff Verlag, Leipzig.</div></li><li id="index.xml-bibl-d30e619"><div class="biblfree"><span style="font-weight:bold">Kafka, </span> <span style="font-weight:bold">F.</span> (1919). In der Strafkolonie. Kurt Wolff Verlag, Leipzig.</div></li><li id="index.xml-bibl-d30e628"><div class="biblfree"><span style="font-weight:bold">Mehler, </span> <span style="font-weight:bold">A.</span> (2008). Large text networks as an object of corpus linguistic studies. In Anke Lüdeling and Merja Kytö, editors, <span style="font-style:italic">Corpus Linguistics. An International Handbook of the Science of Language and Society</span>. De Gruyter, Berlin/New York, pp. 328–82.</div></li><li id="index.xml-bibl-d30e640"><div class="biblfree"><span style="font-weight:bold">Mehler, A., </span> <span style="font-weight:bold">Gleim, R., </span> <span style="font-weight:bold">vor der Brück, T., </span> <span style="font-weight:bold">Hemati, W., </span> <span style="font-weight:bold">Uslu, T. and Eger, S.</span> (2016). “Wikidition: Automatic Lexiconization and Linkiﬁcation of Text Corpora,” <span style="font-style:italic">Information Technology</span>.</div></li><li id="index.xml-bibl-d30e662"><div class="biblfree"><span style="font-weight:bold">Mihalcea, </span> <span style="font-weight:bold">R.</span> <span style="font-weight:bold">and Csomai, </span> <span style="font-weight:bold">A.</span> (2007). Wikify!: Linking documents to encyclopedic knowledge. In <span style="font-style:italic">Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07</span>. New York, NY, USA. ACM, pp. 233–42.</div></li><li id="index.xml-bibl-d30e681"><div class="biblfree"><span style="font-weight:bold">Mikolov, </span> <span style="font-weight:bold">T.,</span> <span style="font-weight:bold">Yih, </span> <span style="font-weight:bold">W. </span> <span style="font-weight:bold">and Zweig, </span> <span style="font-weight:bold">G.</span> (2013). Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, (eds), <span style="font-style:italic">Proceedings of NAACL 2013</span>. The Association for Computational Linguistics, pp. 746–51.</div></li><li id="index.xml-bibl-d30e706"><div class="biblfree"><span style="font-weight:bold">Miller, G. A. and Charles, W. G.</span> (1991). Contextual correlates of semantic similarity. <span style="font-style:italic">Language and Cognitive Processes</span>,<span style="font-weight:bold">6</span>(1): 1–28.</div></li><li id="index.xml-bibl-d30e718"><div class="biblfree"><span style="font-weight:bold">Moretti, </span> <span style="font-weight:bold">F</span>. (2013). Distant Reading. <span style="font-style:italic">Verso</span>.</div></li><li id="index.xml-bibl-d30e730"><div class="biblfree"><span style="font-weight:bold">Paaß, </span> <span style="font-weight:bold">G., </span> <span style="font-weight:bold">Kindermann, </span> <span style="font-weight:bold">J.</span> <span style="font-weight:bold">and Leopold, </span> <span style="font-weight:bold">E.</span> (2004). Learning prototype ontologies by hierarchical latent semantic analysis. In Andreas Abecker, Steffen Bickel, Ulf Brefeld, Isabel Drost, Nicola Henze, Olaf Herden, Mirjam Minor, Tobias Scheffer, Ljiljana Stojanovic, and Stephan Weibelzahl, (eds), <span style="font-style:italic">LWA 2004: Lernen – Wissensentdeckung – Adaptivität</span>. Humbold-Universität Berlin, pp. 193–205.</div></li><li id="index.xml-bibl-d30e755"><div class="biblfree"><span style="font-weight:bold">Raible, </span> <span style="font-weight:bold">W.</span> (1981). Von der Allgegenwart des Gegensinns (und einiger anderer Relationen). Strategien zur Einordnung semantischer Informationen. <span style="font-style:italic">Zeitschrift für romanische Philologie</span>,<span style="font-weight:bold">97</span>(1-2): 1–40.</div></li><li id="index.xml-bibl-d30e770"><div class="biblfree"><span style="font-weight:bold">Rauchberg, </span> <span style="font-weight:bold">H.</span> (1890). Statistische Technik. Deutsche Statistische Gesellschaft, 1.</div></li><li id="index.xml-bibl-d30e780"><div class="biblfree"><span style="font-weight:bold">Thaller, </span> <span style="font-weight:bold">M.</span> (2014). The humanities are about research, first and foremost; their interaction with computer science should be too. <span style="font-style:italic">Dagstuhl Reports</span>,<span style="font-weight:bold">4</span>(7): 108–10.</div></li></ol></div></body></html>
		</div>
		<div class="col-lg-2">
		</div>
	</div>


		</div>
		<script>
		$(document).ready(function(){
			$('[data-toggle="tooltip"]').tooltip();
		});
		</script>			
	</body>
</html>