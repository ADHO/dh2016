<!DOCTYPE html>



<html lang="en">
	<head>
		<meta charset="utf-8">	
		<meta name="viewport" content="width=device-width, initial_scale=1">
		<title>DH 2016 Abstracts</title>
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css">
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="/static/css/abstracts.css">
		<script src="/static/jquery/jquery-2.2.4.min.js"></script>
		<script src="/static/bootstrap/js/bootstrap.min.js"></script>
	</head>
	<body>
		<div class="container">
			<div class="row">
				<a href="http://dh2016.adho.org"><img src="/static/images/logo_4.png" class="img-responsive"></a>
			</div>
			<div class="row">
				<ol class="breadcrumb">
					
						<li><a href="http://www.dh2016.adho.org">DH Home</a></li>
					
						<li><a href="/abstracts/">Abstracts</a></li>
					
						<li><a href="/abstracts/128">128</a></li>
					
				</ol>
			</div>
					
			
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8 col-xs-12">
			<button class="btn btn-default" data-toggle="collapse" data-target="#head" style="margin-bottom: 5px;">Show info</button>
			<button class="btn btn-default" data-toggle="collapse" data-target="#cite" style="margin-bottom: 5px;">How to cite</button>
			<a href="/static/data/243.xml" class="btn btn-default" role="button" style="margin-bottom: 5px;" download="128.xml">XML Version</a>
			<div class="panel panel-success collapse" style="padding: 10px; border=none;" id="head">
				<div>
					<b>Title: </b>ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities
					<br>
					<b>Authors: </b>
					Tanya Clement, Steve McLaughlin, David Tcheng, Loretta Auvil, Tony Borries
					<br>
					<b>Category: </b>Paper:Long Paper
					<br>
					<b>Keywords: </b>Audio user interfaces; data mining; machine learning; infrastructure development; Sound studies
				</div>
			</div>
			<div class="collapse" id="cite">
				<b>Clement, T., McLaughlin, S., Tcheng, D., Auvil, L., Borries, T.</b> (2016). ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities. In <i>Digital Humanities 2016: Conference Abstracts</i>. Jagiellonian University & Pedagogical University, Kraków, 
				
					pp. 158-161.
				
			</div>
		</div>
		<div class="col-lg-2">
		</div>
	</div>
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8">	
			<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)--><title>ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities</title><meta name="author" content="Tanya Clement , Steve McLaughlin , David Tcheng , Loretta Auvil , and Tony Borries" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta name="DC.Title" content="ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities" /><meta name="DC.Type" content="Text" /><meta name="DC.Format" content="text/html" /><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css" /><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css" /></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle">ARLO (Adaptive Recognition with Layered Optimization): a Prototype for High Performance Analysis of Sound Collections in the Humanities</h1></div><!--TEI front--><!--TEI body--><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">Introduction</span></h2><p class="Standard Indent">Beyond simple annotation and visualization tools or expensive proprietary software, open access software for accessing and analyzing audio is not widely available for general use by the humanities community. Speech recognition algorithms in projects such as MALACH (Multilingual Access to Large spoken ArCHives) are often not built as Web-accessible interfaces for broader audiences. Analysis and visualization software such as PRAAT, which is used by linguists, and Sonic Visualizer, which is often used by music scholars, are desktop tools that typically allow users to focus on one file at a time, making project-sharing difficult for collaborative research and classroom projects. In bioacoustics, researchers use Raven (from the Cornell Lab of Ornithology) and Avisoft (expensive, proprietary software), which perform well with clean data from a single animal. Most of these tools are either not used in multiple domains or with large collections and none of them do well with noise or with multiple signals. As a result of these factors, humanists have few opportunities to use advanced technologies for analyzing large, messy sound archives. In response to this lack, the School of Information (iSchool) at the University of Texas at Austin (UT) and the Illinois Informatics Institute (I3) at the University of Illinois at Urbana-Champaign (UIUC) are collaborating on the HiPSTAS (High Performance Sound Technologies for Access and Scholarship) project. A primary goal of HiPSTAS is to develop a research environment that uses machine learning and visualization to automate processes for describing unprocessed spoken word collections of keen interest to humanists. </p><p class="Standard Indent">This paper describes how we have developed, as a result of HiPSTAS, a machine learning system called ARLO (Adaptive Recognition with Layered Optimization) to help deal with the information challenges that scholars encounter in their attempt to do research with unprocessed audio collections.</p></div><div class="DH-Heading1" id="index.xml-body.1_div.2"><h2 class="DH-Heading1"><span class="headingNumber">2. </span><span class="head">ARLO (Adaptive Recognition with Layered Optimization) Software</span></h2><p class="Standard Indent">ARLO was developed with UIUC seed funding for avian ecologist David Enstrom (2008) to begin exploring the use of machine learning for data analysis in the fields of animal behavior and ecology. ARLO software was chosen as the software we would develop through HiPSTAS primarily because it extracts basic prosodic features such as pitch, rhythm, and timbre that humanities scholars have called significant for performing analysis with sound collections (Bernstein, 2011; Sherwood, 2006; Tsur, 1992). </p><div class="DH-Heading2" id="index.xml-body.1_div.2_div.1"><h3 class="DH-Heading2"><span class="headingNumber">2.1. </span><span class="head">Filter Bank Signal Processing and Spectrogram Generation</span></h3><p class="Standard Indent">ARLO analyzes audio by extracting features based on time and frequency information in the form of a spectrogram. The spectrogram is computed using band-pass filters linked with energy detectors. The filter bank approach is similar to using an array of tuning forks, each positioned at a separate frequency, an approach that is thought to best mimic the processes of the human ear (Salthouse and Sarpeshkar). With filter banks, users can optimize the trade-off between time and frequency resolutions in the spectrograms (Rossing, 2001) by choosing a frequency range and ‘damping factor’ (or damping ratio), a parameter that determines how long the tuning forks ‘ring.’ By selecting these features, users can optimize their searches for a given sound. For these reasons, </p></div><div class="DH-Heading2" id="index.xml-body.1_div.2_div.2"><h3 class="DH-Heading2"><span class="headingNumber">2.2. </span><span class="head">Machine-Learning Examples and the ARLO API (Application Programming Interface)</span></h3><p>In ARLO, examples for machine learning are audio events that the user has identified and labeled. Audio events comprise a start and end time such as a two-second clip, as well as an optional minimum and maximum frequency band to isolate the region of interest. Users label the examples of interest (e.g., “applause” or “barking”). Other control parameters such as damping factor are also provided for creating spectrograph data according to optimal resolutions for a given problem. The algorithm described below retrieves the features of the tag according to the user's chosen spectra and framing size (e.g., two frames per second, each 0.5 seconds) from the audio file through the ARLO API.</p></div><div class="DH-Heading2" id="index.xml-body.1_div.2_div.3"><h3 class="DH-Heading2"><span class="headingNumber">2.3. </span><span class="head">ARLO Machine-Learning Algorithms: IBL (Instance-Based Learning)</span></h3><p class="Standard Indent">The ARLO IBL algorithm finds matches by taking each known classified example and “sliding” it across new audio files looking for good matches based on a distance metric. The average of the weighted training set classes determines prediction probability. The number of match positions considered per second is adjustable and is set to the spectral sample rate. In addition to simple spectra matching, a user can isolate pitch and volume traces, compute correlations on them, and weight the different feature types when computing the overall match strength. This allows the user to weight spectral information that might correspond to such aspects as pitch or rhythm. In the IBL algorithm, accuracy is measured using a simulation of the leave-one-out cross-validation prediction process described above. </p></div></div><div class="DH-Heading1" id="index.xml-body.1_div.3"><h2 class="DH-Heading1"><span class="headingNumber">3. </span><span class="head">Use Case: Finding Applause in PennSound Poetry Performances</span></h2><p>Humanities scholars have identified the sound of applause as a significant signpost for finding patterns of interest in recorded poetry performances. Applause can serve as a delimiter between performances, indicating how a file can be segmented and indexed. Applause can also serve as a delimiter between the introduction to a performance and the moment when a performance has ended and a question-and-answer period has begun, both of which indicate contextual information such as the presence of people who might not appear in traditional metadata fields (Clement and McLaughlin, 2015). A means for quantifying the presence of applause can also lead researchers to consider more in-depth studies concerning the relationship between audience responses and a poet’s performance of the same poem at different venues as well as the differing responses of audiences at the same venue over the course of a poet’s career or perhaps as a point of comparison between poets. Examples of these results are described below.</p><p>For this use case, we ingested approximately 30,257 files remaining (5374.89 hours) from PennSound into ARLO. We chose 2,000 files at random, manually examined them for instances of applause, and chose one instance of applause per recording until we had an example training set of 852 three-second tags, including 582 3-second instances of non-applause (3492 0.5-second examples) and 270 3-second instances of applause (1620 0.5-second examples). Optimization for the IBL test went through 100 iterations. As a result of this optimization process, we used the following parameters for both tests: 0.5-second spectral resolution; 0.5 damping factor; 0.8 weighting power (for IBL); 600 Hz minimum frequency; 5000 Hz maximum frequency; 64 (IBL) and 256 (Weka) spectral bands; spectral sampling rate of 2 (i.e., half-second resolution). </p><div class="DH-Heading2" id="index.xml-body.1_div.3_div.1"><h3 class="DH-Heading2"><span class="headingNumber">3.1. </span><span class="head">Preliminary Results</span></h3><p class="Standard Indent">We first evaluated our models using cross-validation on the training data. Using the leave-one-out approach, the IBL classifier achieved an overall accuracy of 94.52% with a 0.5 cut-off classification threshold. After comparing 676 configurations, we found that the optimal approach was using IBL with Hann smoothing over 14 windows (7 seconds). The accuracy for this configuration was 99.41%. </p><p>In our initial analysis of classification data, we identified significant differences between measured applause durations for six poets, each with more than ten readings in the evaluation set. Table 6 presents the results of pairwise single-tailed Mann-Whitney (Mann and Whitney, 1947) <span style="font-style:italic">U</span> tests of applause durations that have been predicted by our IBL classifier. The alternative hypothesis states that the performer in the left column tends to receive more applause than the corresponding one listed in the top row. <span style="color:222222">Results that are significant at the p&lt;0.05 level appear in bold, with the counts and overall means of each set of observations provided in the right two columns. </span>It appears, for instance, that the poet Rae Armantrout tends to receive more applause than either Bruce Andrews or Barrett Watten. These two differences remain significant when comparing "seconds of applause per minute" instead of total applause duration.</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/243/image1.png" alt="Table 1. P-values for Pairwise Directional Mann-Whitney &#xA;                             Tests Between Six Poets' Applause Durations&#xA;                        " class="inline" /><div class="caption">Table 1. P-values for Pairwise Directional Mann-Whitney <span style="font-style:italic">U</span> Tests Between Six Poets' Applause Durations</div></div></div></div><div class="DH-Heading1" id="index.xml-body.1_div.4"><h2 class="DH-Heading1"><span class="headingNumber">4. </span><span class="head">Discussion and Future Work</span></h2><p class="Standard Indent">This is preliminary work in an ongoing attempt to create a virtual research environment for analyzing large collections of audio. These data warrant further scrutiny, however, since multiple factors might be skewing the results. First, recording technologies have changed over time and as a result some earlier recordings likely include more noise and thus more false positives. Second, editing practices and event formats can vary widely between venues and over time. Finally, recordings that are included in the PennSound archive represent curation decisions that may favor certain kinds of performers over others. Part of the challenge is to determine what use such analysis might serve for scholars in the humanities and in other fields. Some of the HiPSTAS participants have written about their experiences using ARLO in their research (MacArthur, 2015; Mustazza, 2015; Sherwood, 2015; Rettberg, 2015), but we are interested in feedback from multiple user communities including linguists and scientists who have recorded too much sound data for traditional forms of analysis and processing (Servick, 2014). Furthermore, the IBL algorithm produced promising results, but could be improved with more training data, for example. Or, in extended experiments in which users wish to increase the accuracy of the model, we could develop a voting mechanism on the predictions by comparing the models. Users could validate newly identified examples and include them as new training examples, building each model again on the new data. We are currently working on further testing the models and developing a means for these iterative approaches. </p></div><!--TEI back--><div class="bibliogr" id="index.xml-back.1_div.1"><h2><span class="headingNumber"> </span></h2><div class="listhead">Bibliography</div><ol class="listBibl"><li id="index.xml-bibl-d30e324"><div class="References"><span style="font-weight:bold">Bernstein, C.</span> (2011). <span style="font-style:italic">Attack of the Difficult Poems: Essays and Inventions</span>. Chicago: University Of Chicago Press.</div></li><li id="index.xml-bibl-d30e332"><div class="References"><span style="font-weight:bold">Bioacoustics Research Program.</span> (2014). Raven Pro: Interactive Sound Analysis Software (Version 1.5). Ithaca, NY: The Cornell Lab of Ornithology.</div></li><li id="index.xml-bibl-d30e337"><div class="References"><span style="font-weight:bold">Boersma, P.</span> (2001). Praat, a system for doing phonetics by computer.  <span style="font-style:italic">Glot International</span>, <span style="font-weight:bold">5</span>(9/10): 341-45.</div></li><li id="index.xml-bibl-d30e348"><div class="References"><span style="font-weight:bold">Cannam, C., Landone, C. and Sandler, M.</span> (2010). Sonic Visualiser: An Open Source Application for Viewing, Analysing, and Annotating Music Audio Files. In <span style="font-style:italic">Proceedings of the ACM Multimedia 2010 International Conference</span>.</div></li><li id="index.xml-bibl-d30e357"><div class="References"><span style="font-weight:bold">Council on Library and Information Resources and the Library of Congress</span>. (2010). <span style="font-style:italic">The State of Recorded Sound Preservation in the United States: A National Legacy at Risk in the Digital Age</span>. Washington DC: National Recording Preservation Board of the Library of Congress.</div></li><li id="index.xml-bibl-d30e365"><div class="References"><span style="font-weight:bold">Enstrom, D. A. and Ward, M. P.</span> (2008). Sex specific song repertoires in Northern Cardinals: mutual assessment and the occurrence of female song. <span style="font-style:italic">The 12 International Behavioral Ecology Meetings</span>, Ithaca, NY.</div></li><li id="index.xml-bibl-d30e373"><div class="References"><span style="font-weight:bold">Greene, M. A. and Meissner, D.</span> (2005). More Product, Less Process: Revamping Traditional Archival Processing. <span style="font-style:italic">The American Archivist</span>, <span style="font-weight:bold">68</span>(2): 208–63.</div></li><li id="index.xml-bibl-d30e384"><div class="References"><span style="font-weight:bold">Hall, M., et al.</span> (2009). The WEKA Data Mining Software: An Update. <span style="font-style:italic">SIGKDD Explorations</span>, <span style="font-weight:bold">11</span>(1).</div></li><li id="index.xml-bibl-d30e395"><div class="References"><span style="font-weight:bold">MacArthur, M.</span> (2015). Monotony, the Churches of Poetry Reading, and Sound Studies. <span style="font-style:italic">PMLA</span>. Forthcoming.</div></li><li id="index.xml-bibl-d30e403"><div class="References"><span style="font-weight:bold">Mann, H. B. and Whitney, D. R.</span> (1947). On a Test of Whether One of Two Random Variables is Stochastically Larger than the Other. <span style="font-style:italic">The Annals of Mathematical Statistics</span>, <span style="font-weight:bold">18</span>(1): 50–60.</div></li><li id="index.xml-bibl-d30e415"><div class="References"><span style="font-weight:bold">Mustazza, C.</span> (2015) The noise is the content: Toward computationally determining the provenance of poetry recordings using ARLO. <span style="font-style:italic">Jacket2</span>. Retrieved from https://jacket2.org/commentary/noise-content-toward-computationallydetermining-provenance-poetry-recordings</div></li><li id="index.xml-bibl-d30e423"><div class="References"><span style="font-weight:bold">Nelson-Strauss, B., Gevinson, A. and Brylawski, S.</span> (2012). The Library of Congress National Recording Preservation Plan. Washington, DC: Library of Congress.</div></li><li id="index.xml-bibl-d30e428"><div class="References"><span style="font-weight:bold">Powers, D. M. W.</span> (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness and Correlation. <span style="font-style:italic">Journal of Machine Learning Technologies</span>, <span style="font-weight:bold">2</span>(1): 37–63.</div></li><li id="index.xml-bibl-d30e439"><div class="References"><span style="font-weight:bold">Rettberg, E.</span> (2015). Hearing the Audience. <span style="font-style:italic">Jacket2</span>, <span style="font-weight:bold">26</span>. Retrieved from <a class="link_ref" href="http://jacket2.org/commentary/hearing-audience">http://jacket2.org/commentary/hearing-audience</a>.</div></li><li id="index.xml-bibl-d30e453"><div class="References"><span style="font-weight:bold">R Core Team</span> (2014). R: A language and environment for statistical computing. <span style="font-style:italic">R Foundation for Statistical Computing</span>, Vienna, Austria. Retrieved from <a class="link_ref" href="http://www.r-project.org/">http://www.R-project.org</a></div></li><li id="index.xml-bibl-d30e465"><div class="References"><span style="font-weight:bold">Rossing, T. D., and Moore, F. R.</span> (2001). <span style="font-style:italic">The Science of Sound</span> <span style="font-weight:bold">(3rd edition.)</span>, San Francisco: Addison-Wesley.</div></li><li id="index.xml-bibl-d30e477"><div class="References"><span style="font-weight:bold">Salthouse, C. D. and Sarpeshkar, R.</span> (2003). A Practical Micropower Programmable Bandpass Filter for Use in Bionic Ears. <span style="font-style:italic">IEEE Journal Of Solid-State Circuits</span>, <span style="font-weight:bold">38</span>(1): 63-70.</div></li><li id="index.xml-bibl-d30e488"><div class="References"><span style="font-weight:bold">Servick, K.</span> (2014). Eavesdropping on Ecosystems. <span style="font-style:italic">Science Magazine</span>. doi: 343.6173: 834–837.</div></li><li id="index.xml-bibl-d30e496"><div class="References"><span style="font-weight:bold">Sherwood, K.</span> (2006). Elaborate Versionings: Characteristics of Emergent Performance in Three Print/Oral/ Aural Poets. In <span style="font-style:italic">Oral Tradition</span>, <span style="font-weight:bold">21</span>(1): 119-47.</div></li><li id="index.xml-bibl-d30e507"><div class="References"><span style="font-weight:bold">Sherwood, K.</span> (2015). Distanced sounding: ARLO as a tool for the analysis and visualization of versioning phenomena within poetry audio. <span style="font-style:italic">Jacket2</span>. Retrieved from https://jacket2.org/commentary/distanced-sounding-arlo-toolanalysis-and-visualization-versioning-phenomena-within-poetr.</div></li><li id="index.xml-bibl-d30e515"><div class="References"><span style="font-weight:bold">Smith, A., Allen, D. R. and Allen, K.</span> (2004). Survey of the State of Audio Collections in Academic Libraries. Washington, DC: Council on Library and Information Resources.</div></li><li id="index.xml-bibl-d30e520"><div class="References"><span style="font-weight:bold">Tsur, R.</span> (1992). <span style="font-style:italic">What Makes Sound Patterns Expressive?: The Poetic Mode of Speech Perception</span>. Duke University Press.</div></li></ol></div></body></html>
		</div>
		<div class="col-lg-2">
		</div>
	</div>


		</div>
		<script>
		$(document).ready(function(){
			$('[data-toggle="tooltip"]').tooltip();
		});
		</script>			
	</body>
</html>