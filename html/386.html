<!DOCTYPE html>



<html lang="en">
	<head>
		<meta charset="utf-8">	
		<meta name="viewport" content="width=device-width, initial_scale=1">
		<title>DH 2016 Abstracts</title>
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css">
		<link rel="stylesheet" href="/static/bootstrap/css/bootstrap-theme.min.css">
		<link rel="stylesheet" href="/static/css/abstracts.css">
		<script src="/static/jquery/jquery-2.2.4.min.js"></script>
		<script src="/static/bootstrap/js/bootstrap.min.js"></script>
	</head>
	<body>
		<div class="container">
			<div class="row">
				<a href="http://dh2016.adho.org"><img src="/static/images/logo_4.png" class="img-responsive"></a>
			</div>
			<div class="row">
				<ol class="breadcrumb">
					
						<li><a href="http://www.dh2016.adho.org">DH Home</a></li>
					
						<li><a href="/abstracts/">Abstracts</a></li>
					
						<li><a href="/abstracts/386">386</a></li>
					
				</ol>
			</div>
					
			
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8 col-xs-12">
			<button class="btn btn-default" data-toggle="collapse" data-target="#head" style="margin-bottom: 5px;">Show info</button>
			<button class="btn btn-default" data-toggle="collapse" data-target="#cite" style="margin-bottom: 5px;">How to cite</button>
			<a href="/static/data/376.xml" class="btn btn-default" role="button" style="margin-bottom: 5px;" download="386.xml">XML Version</a>
			<div class="panel panel-success collapse" style="padding: 10px; border=none;" id="head">
				<div>
					<b>Title: </b>Music notation addressability
					<br>
					<b>Authors: </b>
					Raffaele Viglianti
					<br>
					<b>Category: </b>Paper:Long Paper
					<br>
					<b>Keywords: </b>API, citation, text and music, music notation, web
				</div>
			</div>
			<div class="collapse" id="cite">
				<b>Viglianti, R.</b> (2016). Music notation addressability. In <i>Digital Humanities 2016: Conference Abstracts</i>. Jagiellonian University & Pedagogical University, Kraków, 
				
					pp. 398-400.
				
			</div>
		</div>
		<div class="col-lg-2">
		</div>
	</div>
	<div class="row">
		<div class="col-lg-2">
		</div>
		<div class="col-lg-8">	
			<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><!--THIS FILE IS GENERATED FROM AN XML MASTER. DO NOT EDIT (5)--><title>Music notation addressability </title><meta name="author" content="Raffaele Viglianti" /><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets" /><meta name="DC.Title" content="Music notation addressability" /><meta name="DC.Type" content="Text" /><meta name="DC.Format" content="text/html" /><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css" /><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css" /></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle"><span class="titlem">Music notation addressability</span> <span class="titlem"></span></h1></div><!--TEI front--><!--TEI body--><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">Introduction</span></h2><p>How can one virtually ‘circle’ some music notation as one would on a printed score? How can a machine interpret this ‘circling’ to select and retrieve the relevant music notation in digital format? This paper will introduce the concept of addressability for music notation, on the basis of a comparison with textual addressability as defined by Michael Witmore (2010). Additionally, the paper will report on the work of <span style="font-style:italic">Enhancing Music notation Addressability</span> (EMA), a NEH-funded one-year project that has developed methods for addressing arbitrary portions of encoded music notation on the web.</p><p>Many Digital Humanities projects are concerned with the digitization of cultural objects for varied purposes of study and dissemination. Theorists such as Willard McCarty (2005) and Julia Flanders (2009) have highlighted the fact that digitization involves the creation of a data model of a cultural object, whereby scholarly interpretation and analysis is inevitably included in the model. Editorial projects in literary studies, for example, often model sources by encoding transcription and editorial intervention with the Text Encoding Initiative (TEI) format. The ability to identify and name textual structures is a fundamental operation in the creation of such models. Michael Witmore has called text a “massively addressable object” (2010); that is, given certain abstractions and conventions, it is possible to identify areas of a text such as characters, words, as well as chapters or proper names. Reading practices influence and contribute to the development of such conventions and abstractions, but, Witmore argues, addressability is a textual condition regardless of technology. With digital texts, modes of address become more abstract, so that arbitrary taxonomies can be identified as well as more established ones. To exemplify a more abstract mode of address, Witmore suggests items “identified as a ‘History’ by Heminges and Condell in the First Folio”. This enhanced addressability available in a digital context is the engine for textual analysis and scholarly discourse about digital text.</p><p>This idea of addressability is arguably applicable to many more kinds of “text”, including music notation; indeed, addressing units of music notation (such as measures, notes, and phrases) has long been a powerful instrument in musicology for both analysis and historical narrative. <span id="ftn1_return"><a class="notelink" title="When talking about music in general, it is important to say that addressing written music notation is not the only instrument of the musicologist. Mus…" href="#ftn1"><sup>1</sup></a></span> Music notation, however, is more complicated to represent digitally than text. Human-computer interaction has since its early days been built around the concept of character and line, which makes dealing with “plain” text a fairly straightforward matter for many basic operations; counting the number of characters in a given plain text document is trivial in any digital environment. <span id="ftn2_return"><a class="notelink" title="Modern computing systems are able to support complex ancient and modern writing systems, including those requiring right-to-left strings and compound …" href="#ftn2"><sup>2</sup></a></span> Music notation, on the other hand, requires substantial computational modelling even for the simplest musical text before any further operation is possible. This is particularly evident when music notation is represented with markup, which implies a system based on characters and lines. There are many different ways of representing a single note; some aspects are common to all representation systems, such as information about pitch and duration, but some systems will prioritize certain aspects over others. To give a simple example, one system may represent beams (ligatures between flagged notes, usually shorter in duration), while others may ignore them altogether. <span id="ftn3_return"><a class="notelink" title="By grouping notes together, beams provide important—but somewhat secondary to pitch and duration—information to the reader of a music score, such as a…" href="#ftn3"><sup>3</sup></a></span></p><p>Nonetheless, there are simple units that are typically represented by all music notation systems for common western music notation, such as measure, staff (or instrument), and beat. The EMA project, therefore, developed a URI scheme and an Application Programming Interface (API) to make it possible to target music notation resources on the web regardless of their format. Such a scheme may facilitate (and in some cases enable) a number of activities around music notation documents published on the web. The following table gives a few basic examples of how an implementation of the URI scheme could be useful to musicological research:</p><div class="table" id="Table1"><table class="frame" style="border-collapse:collapse;border-spacing:0;"><tr><td class="bold">Scholarly</td><td class="bold">Visual</td><td class="bold">Procedural</td></tr><tr><td><span style="font-style:italic">Analysis</span>: being able to address components of music notation for analytical purposes. Example: precisely identify start and end of a pedal tone in Bach’s <span style="font-style:italic">Prelude no. 6 in D Minor</span>, BWV 851.</td><td><span style="font-style:italic">Rendering</span>: rendering music notation in an interactive environment such as a browser or a tablet requires the ability to cut up a large music document. For example to show only the number of measures that fit in a given space.</td><td><span style="font-style:italic">Processing</span>: extracted portions of music notation can be passed on to another process. For example, given the MEI encoding of the Overture to Mozart’s <span style="font-style:italic">Don </span> <span style="font-style:italic">Giovanni</span>, extract the string instrument parts and send them to another program that will return an harmonic analysis.</td></tr><tr><td><span style="font-style:italic">Citation</span>: quote a passage from an encoded music notation file. For example the timpani in the opening bars of the Overture to Mozart’s Don Giovanni.</td><td><span style="font-style:italic">Highlighting</span>: address a segment of music notation to highlight it in a visual context (e.g. with color).</td><td></td></tr></table></div><p>The EMA project has particularly focused on facilitating citation and attribution of credit, as is discussed in the “Evaluation” section below.</p></div><div class="DH-Heading1" id="index.xml-body.1_div.2"><h2 class="DH-Heading1"><span class="headingNumber">2. </span><span class="head">A brief overview of the specification</span></h2><p>The specification was created to provide a web-friendly mechanism for addressing specific portions of music notation in digital format. This is not unlike the APIs often provided by image servers for retrieving specific portions of an image. Such servers typically operate on a given large image ﬁle and are able to return different zoom levels and coordinate spaces. The International Image Interoperability Framework (IIIF) has recently created an API to generalize interaction with image providers, so that it can be implemented across multiple servers and digital libraries. IIIF was used as a model for the Music Addressability API created for EMA and briefly described here.</p><p>Consider the following example, <span id="ftn4_return"><a class="notelink" title="Taken from Du Chemin: Lost Voices project, at http://digitalduchemin.org." href="#ftn4"><sup>4</sup></a></span> and the notation highlighted in the boxes:</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/376/10000000000004E30000024A46E9FC9C.png" alt="" class="graphic" /></div><p>The highlighted notation occurs between measure 38 and 39, on the first and third staves (labelled <span style="font-style:italic">Superius</span> and <span style="font-style:italic">Tenor</span> — this is a renaissance choral piece). Measure 38, however, is not considered in full, but only starting from the third beat. This selection can be expressed according to a URI syntax:</p><p>/{identifier}/{measures}/{staves}/{beats}/</p><p>/dc0519.mei/38-39/1,3/@3-3</p><p>The measure is expressed as a range (38-39), staves can be selected through a range or separately with a comma (1,3), and the beats are always relative to their measure, so @3-3 means the third beat of the starting measure to the third beat of the ending measure. <span id="ftn5_return"><a class="notelink" title="A complete description of the URI scheme and the API is available at: ." href="#ftn5"><sup>5</sup></a></span> In this specification the beat is the primary driver of the selection: it allows for precise addressability of contiguous as well as non-contiguous areas.</p><p>Music notation, however, occasionally breaks rules in favor of flexibility. Cadenzas, for example, are ornamental passages of an improvisational nature that can be written out with notation that disregards a measure’s beat, making it impossible to address subsets of the cadenza wit the syntax discussed above. While EMA’s URI scheme offers the granularity sufficient to address the vast majority of western music notation, a necessary future improvement on the API is, indeed, an extension that would make it possible to address music notation with more flexible beat.</p></div><div class="DH-Heading1" id="index.xml-body.1_div.3"><h2 class="DH-Heading1"><span class="headingNumber">3. </span><span class="head">Evaluation</span></h2><p>In order to evaluate the specification, EMA has created an implementation of the API as a web service. While the URI specification can be absolute from a specific representation, the implementation must know how to operate on specific formats. The web service that we coded operates on the The Music Encoding Initiative format and is called Open MEI Addressability Service (Omas). <span id="ftn6_return"><a class="notelink" title="A demo is available at ." href="#ftn6"><sup>6</sup></a></span> Omas interprets a conformant URI, retrieves the specified MEI resource, applies the selection, and returns it. An additional parameter on the URI can be used to determine how “complete” the retrieved selection should be (whether it should, for example, include time and key signatures, etc.).</p><div class="panel panel-default panel-figure"><img class="img-responsive" src="/static/data/376/100000000000049D000000F2163CE9F5.png" alt="" class="graphic" /></div><p>Similarly to an image server, Omas assumes that the information specified by the URL can be retrieved in the target MEI file. If requested, the web service can return metadata information about an MEI file, such as number of measures, staves, beats and their changes throughout the document. This can be used to facilitate the creation of URL requests able to return the selection required.</p><p>Finally, EMA partnered with the <span style="font-style:italic">Du Chemin: </span> <span style="font-style:italic">Lost Voices</span> project to model a number of micro-analyses addressing music notation from their existing collection of MEI documents. In a second phase of the project, the analyses have been re-modeled as Linked Open Data according to the Nanopublication guidelines. <span id="ftn7_return"><a class="notelink" title="Nanopublication is an ontology for publishing scientific data: http://nanopub.org. The Nanopublication server for Du Chemin: Lost Voices is available …" href="#ftn7"><sup>7</sup></a></span> Each EMA nanopublication addresses an arbitrary portion of music notation using the URL specification described here. Omas operates as a web service to connect the nanopublications with the collection of MEI files in <span style="font-style:italic">Du Chemin</span>.</p></div><!--TEI back--><div class="bibliogr" id="index.xml-back.1_div.1"><h2><span class="headingNumber"> </span></h2><div class="listhead">Bibliography</div><ol class="listBibl"><li id="index.xml-bibl-d30e379"><div class="biblfree"><span style="font-weight:bold">Babbit, M.</span> (1965). The use of computers in musicological research. <span style="font-style:italic">Perspectives of New Music</span>, 3(2): pp. 74–83.</div></li><li id="index.xml-bibl-d30e388"><div class="biblfree"><span style="font-weight:bold">Flanders, J.</span> (2009). Data and Wisdom: Electronic Editing and the Quantiﬁcation of Knowledge. <span style="font-style:italic">Literary and Linguistic Computing</span>, 24(1): pp. 53–62.</div></li><li id="index.xml-bibl-d30e397"><div class="biblfree"><span style="font-weight:bold">McCarty, W.</span> (2005). Chapter 1 - Modelling. <span style="font-style:italic">Humanities Computing</span>, London: Palgrave Macmillan.</div></li><li id="index.xml-bibl-d30e406"><div class="biblfree"><span style="font-weight:bold">Witmore, M.</span> (2010). <span style="font-style:italic">Text: A Massively Addressable Object</span>. <a class="link_ptr" href="http://winedarksea.org/?p=926"><span>http://winedarksea.org/?p=926</span></a>.</div></li></ol></div><!--Notes in [TEI]--><div class="notes"><div class="noteHeading">Notes</div><div class="note" id="ftn1"><span class="noteLabel">1. </span><div class="noteBody">When talking about music in general, it is important to say that addressing written music notation is not the only instrument of the musicologist. Music exists on several domains besides the written or "graphemic" one, each addressable in its own way (see Babbitt 1965). For the purpose of this paper, we focus on written Western music notation, because it shares features with written language and for its prominent role in musicological discourse.</div></div><div class="note" id="ftn2"><span class="noteLabel">2. </span><div class="noteBody">Modern computing systems are able to support complex ancient and modern writing systems, including those requiring right-to-left strings and compound symbols. The Unicode Consortium has been at the forefront of the internationalization of computing systems. Nonetheless, computationally speaking, a “string” of text remains a sequence of characters even in more complex representations. Indeed, many compound Unicode characters still retain sequentiality, i.e. one component comes after the other and the compound symbol only makes sense if they are in the correct order. Music notation is not a string of text; therefore this is not possible.</div></div><div class="note" id="ftn3"><span class="noteLabel">3. </span><div class="noteBody">By grouping notes together, beams provide important—but somewhat secondary to pitch and duration—information to the reader of a music score, such as a performer, a musicologist, or an algorithm.</div></div><div class="note" id="ftn4"><span class="noteLabel">4. </span><div class="noteBody">Taken from <span style="font-style:italic">Du Chemin:</span> <span style="font-style:italic">Lost Voices</span> project, at <a class="link_ref" href="http://digitalduchemin.org/">http://digitalduchemin.org</a>.</div></div><div class="note" id="ftn5"><span class="noteLabel">5. </span><div class="noteBody">A complete description of the URI scheme and the API is available at: <a class="link_ptr" href="https://github.com/umd-mith/ema/blob/master/docs/api.md"><span>https://github.com/umd-mith/ema/blob/master/docs/api.md</span></a>.</div></div><div class="note" id="ftn6"><span class="noteLabel">6. </span><div class="noteBody">A demo is available at <a class="link_ptr" href="http://mith.us/ema/omas/"><span>http://mith.us/ema/omas/</span></a>.</div></div><div class="note" id="ftn7"><span class="noteLabel">7. </span><div class="noteBody">Nanopublication is an ontology for publishing scientific data: <a class="link_ref" href="http://nanopub.org/">http://nanopub.org</a>. The Nanopublication server for <span style="font-style:italic">Du Chemin: Lost Voices</span> is available at: <a class="link_ptr" href="http://digitalduchemin.org/np/"><span>http://digitalduchemin.org/np/</span></a>.</div></div></div></body></html>
		</div>
		<div class="col-lg-2">
		</div>
	</div>


		</div>
		<script>
		$(document).ready(function(){
			$('[data-toggle="tooltip"]').tooltip();
		});
		</script>			
	</body>
</html>